{"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/Justus-coded/DPhi-Deep-Learning-Bootcamp/blob/master/Linear_Regression_with_tf_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github","colab_type":"text","cell_id":"00000-538dc9b8-6569-4452-a2ef-04a5497dfd97"}},{"cell_type":"markdown","source":"# Agenda\n1. About the Dataset\n2. Objective\n3. Loading Libraries\n4. Loading Data\n5. View Data\n6. Separate Input Features and Output Features/Variables\n7. Split The Data into Train and Test Set\n8. Train the model (The five step model life cycle)\n  1. Define the model.\n  2. Compile the model.\n  3. Fit the model.\n  4. Evaluate the model\n    * Hyperparameter Tunning\n  5. Prediction","metadata":{"id":"tmHsf5DWsxr6","colab_type":"text","cell_id":"00001-3ccebbae-81a8-481a-834d-34cf8544ddb9"}},{"cell_type":"markdown","source":"## About the Dataset\nWe will be working on a data set that comes from the real estate industry in Boston (US). This database contains 14 attributes. The output variable refers to the median value of owner-occupied homes in 1000 USD's.\n\n* CRIM: per capita crime rate by town\n* ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n* INDUS: proportion of non-retail business acres per town\n* CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n* NOX: nitric oxides concentration (parts per 10 million)\n* RM: average number of rooms per dwelling\n* AGE: proportion of owner-occupied units built prior to 1940\n* DIS: weighted distances to five Boston employment centres\n* RAD: index of accessibility to radial highways\n* TAX: full-value property-tax rate per 10,000 USD\n* PTRATIO: pupil-teacher ratio by town\n* B: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n* LSTAT: lower status of the population (%)\n* MEDV: Median value of owner-occupied homes in 1000 USD's (Output/Target)\n","metadata":{"id":"NmP7x85xcxkS","colab_type":"text","cell_id":"00002-024377a4-740a-49fe-a7ad-83c47f71ed46"}},{"cell_type":"markdown","source":"## Objective\nThe objective is to use linear regression to find the median value of owner-occupied homes in 1000 USD's.\n\nWe will build a Machine learning model (i.e. Linear Regression) using `tensorflow.keras` (in short `tf.keras`) API.","metadata":{"id":"w_c7y3fqWuDX","colab_type":"text","cell_id":"00003-32e7837b-5637-493b-8ef5-f1aa628e29ff"}},{"cell_type":"markdown","source":"You already have all the information about Tensorflow and Keras from earlier modules.","metadata":{"id":"hvnf8jDeTTvw","colab_type":"text","cell_id":"00004-6859c218-cab7-4d48-84fe-2e9df3bc88c0"}},{"cell_type":"markdown","source":"## Loading Libraries\nAll Python capabilities are not loaded to our working environment by default (even if they are already installed in your system). So, we import each and every library that we want to use.\n\nIn data science, numpy and pandas are most commonly used libraries. Numpy is required for calculations like means, medians, square roots, etc. Pandas is used for data processing and working with DataFrames. Matplotlib is used for data visualization. We chose alias names for our libraries for the sake of our convenience (numpy --> np and pandas --> pd, matplotlib.pyplot as plt).\n\n**pyplot:** pyplot is matplotlib's plotting framework. It is the most used module of matplotlib.","metadata":{"id":"PQeegjzCbXdf","colab_type":"text","cell_id":"00005-16d143f0-6666-4eca-81d6-e0eb6199ee05"}},{"cell_type":"code","metadata":{"id":"UEWLuXd7BMrU","colab_type":"code","colab":{},"cell_id":"00006-41c111d9-0850-4c4b-9c72-c32e657a997a"},"source":"# importing packages\nimport numpy as np # to perform calculations \nimport pandas as pd # to read data\nimport matplotlib.pyplot as plt # to visualise","execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Loading Data\nPandas module is used for reading files. We have our data in '.csv' format. We will use 'read_csv()' function for loading the data.","metadata":{"id":"fY_eewx0dhHt","colab_type":"text","cell_id":"00007-5acbf6fe-524c-42df-9f48-e0a2fa44892c"}},{"cell_type":"code","metadata":{"id":"7zlSaTd2W9rt","colab_type":"code","colab":{},"cell_id":"00008-df9ceca7-9492-4610-9303-d24c4041484b"},"source":"# In read_csv() function, we have passed the location to where the file is located at dphi official github page\nboston_data = pd.read_csv(\"https://raw.githubusercontent.com/dphi-official/Datasets/master/Boston_Housing/Training_set_boston.csv\" )","execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## View Data","metadata":{"id":"wze8jmpKtBG4","colab_type":"text","cell_id":"00009-222b51dc-53e0-4f33-88b3-f9ef752ecde6"}},{"cell_type":"code","metadata":{"id":"Nwj5kkDjjQkx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":197},"outputId":"601117bf-cfab-472e-8774-484370e3d4a9","cell_id":"00010-48bd2736-728d-4a6d-b7d1-6ada9b994cff"},"source":"boston_data.head()","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"application/vnd.deepnote.dataframe.v2+json":{"row_count":5,"column_count":14,"columns":[{"name":"CRIM","dtype":"float64","stats":{"unique_count":5,"nan_count":0,"min":0.03466,"max":15.0234,"histogram":[{"bin_start":0.03466,"bin_end":1.533534,"count":3},{"bin_start":1.533534,"bin_end":3.032408,"count":0},{"bin_start":3.032408,"bin_end":4.531282,"count":0},{"bin_start":4.531282,"bin_end":6.030156,"count":0},{"bin_start":6.030156,"bin_end":7.52903,"count":1},{"bin_start":7.52903,"bin_end":9.027904000000001,"count":0},{"bin_start":9.027904000000001,"bin_end":10.526778,"count":0},{"bin_start":10.526778,"bin_end":12.025652000000001,"count":0},{"bin_start":12.025652000000001,"bin_end":13.524526000000002,"count":0},{"bin_start":13.524526000000002,"bin_end":15.0234,"count":1}]}},{"name":"ZN","dtype":"float64","stats":{"unique_count":2,"nan_count":0,"min":0,"max":35,"histogram":[{"bin_start":0,"bin_end":3.5,"count":4},{"bin_start":3.5,"bin_end":7,"count":0},{"bin_start":7,"bin_end":10.5,"count":0},{"bin_start":10.5,"bin_end":14,"count":0},{"bin_start":14,"bin_end":17.5,"count":0},{"bin_start":17.5,"bin_end":21,"count":0},{"bin_start":21,"bin_end":24.5,"count":0},{"bin_start":24.5,"bin_end":28,"count":0},{"bin_start":28,"bin_end":31.5,"count":0},{"bin_start":31.5,"bin_end":35,"count":1}]}},{"name":"INDUS","dtype":"float64","stats":{"unique_count":3,"nan_count":0,"min":6.06,"max":18.1,"histogram":[{"bin_start":6.06,"bin_end":7.263999999999999,"count":1},{"bin_start":7.263999999999999,"bin_end":8.468,"count":2},{"bin_start":8.468,"bin_end":9.672,"count":0},{"bin_start":9.672,"bin_end":10.876000000000001,"count":0},{"bin_start":10.876000000000001,"bin_end":12.080000000000002,"count":0},{"bin_start":12.080000000000002,"bin_end":13.284,"count":0},{"bin_start":13.284,"bin_end":14.488,"count":0},{"bin_start":14.488,"bin_end":15.692,"count":0},{"bin_start":15.692,"bin_end":16.896,"count":0},{"bin_start":16.896,"bin_end":18.1,"count":2}]}},{"name":"CHAS","dtype":"float64","stats":{"unique_count":1,"nan_count":0,"min":0,"max":0,"histogram":[{"bin_start":-0.5,"bin_end":-0.4,"count":0},{"bin_start":-0.4,"bin_end":-0.3,"count":0},{"bin_start":-0.3,"bin_end":-0.19999999999999996,"count":0},{"bin_start":-0.19999999999999996,"bin_end":-0.09999999999999998,"count":0},{"bin_start":-0.09999999999999998,"bin_end":0,"count":0},{"bin_start":0,"bin_end":0.10000000000000009,"count":5},{"bin_start":0.10000000000000009,"bin_end":0.20000000000000007,"count":0},{"bin_start":0.20000000000000007,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0}]}},{"name":"NOX","dtype":"float64","stats":{"unique_count":3,"nan_count":0,"min":0.4379,"max":0.614,"histogram":[{"bin_start":0.4379,"bin_end":0.45551,"count":1},{"bin_start":0.45551,"bin_end":0.47312,"count":0},{"bin_start":0.47312,"bin_end":0.49073,"count":0},{"bin_start":0.49073,"bin_end":0.50834,"count":0},{"bin_start":0.50834,"bin_end":0.52595,"count":0},{"bin_start":0.52595,"bin_end":0.54356,"count":2},{"bin_start":0.54356,"bin_end":0.56117,"count":0},{"bin_start":0.56117,"bin_end":0.57878,"count":0},{"bin_start":0.57878,"bin_end":0.59639,"count":0},{"bin_start":0.59639,"bin_end":0.614,"count":2}]}},{"name":"RM","dtype":"float64","stats":{"unique_count":5,"nan_count":0,"min":5.303999999999999,"max":6.103,"histogram":[{"bin_start":5.303999999999999,"bin_end":5.3839,"count":1},{"bin_start":5.3839,"bin_end":5.463799999999999,"count":0},{"bin_start":5.463799999999999,"bin_end":5.543699999999999,"count":0},{"bin_start":5.543699999999999,"bin_end":5.6236,"count":0},{"bin_start":5.6236,"bin_end":5.7035,"count":0},{"bin_start":5.7035,"bin_end":5.783399999999999,"count":1},{"bin_start":5.783399999999999,"bin_end":5.8633,"count":1},{"bin_start":5.8633,"bin_end":5.9432,"count":0},{"bin_start":5.9432,"bin_end":6.0230999999999995,"count":0},{"bin_start":6.0230999999999995,"bin_end":6.103,"count":2}]}},{"name":"AGE","dtype":"float64","stats":{"unique_count":5,"nan_count":0,"min":23.3,"max":97.3,"histogram":[{"bin_start":23.3,"bin_end":30.700000000000003,"count":1},{"bin_start":30.700000000000003,"bin_end":38.1,"count":0},{"bin_start":38.1,"bin_end":45.5,"count":0},{"bin_start":45.5,"bin_end":52.900000000000006,"count":0},{"bin_start":52.900000000000006,"bin_end":60.3,"count":1},{"bin_start":60.3,"bin_end":67.7,"count":0},{"bin_start":67.7,"bin_end":75.10000000000001,"count":1},{"bin_start":75.10000000000001,"bin_end":82.5,"count":0},{"bin_start":82.5,"bin_end":89.9,"count":1},{"bin_start":89.9,"bin_end":97.3,"count":1}]}},{"name":"DIS","dtype":"float64","stats":{"unique_count":5,"nan_count":0,"min":2.0218,"max":6.6407,"histogram":[{"bin_start":2.0218,"bin_end":2.4836899999999997,"count":2},{"bin_start":2.4836899999999997,"bin_end":2.9455799999999996,"count":0},{"bin_start":2.9455799999999996,"bin_end":3.40747,"count":0},{"bin_start":3.40747,"bin_end":3.86936,"count":1},{"bin_start":3.86936,"bin_end":4.33125,"count":0},{"bin_start":4.33125,"bin_end":4.79314,"count":1},{"bin_start":4.79314,"bin_end":5.25503,"count":0},{"bin_start":5.25503,"bin_end":5.71692,"count":0},{"bin_start":5.71692,"bin_end":6.17881,"count":0},{"bin_start":6.17881,"bin_end":6.6407,"count":1}]}},{"name":"RAD","dtype":"float64","stats":{"unique_count":3,"nan_count":0,"min":1,"max":24,"histogram":[{"bin_start":1,"bin_end":3.3,"count":1},{"bin_start":3.3,"bin_end":5.6,"count":2},{"bin_start":5.6,"bin_end":7.8999999999999995,"count":0},{"bin_start":7.8999999999999995,"bin_end":10.2,"count":0},{"bin_start":10.2,"bin_end":12.5,"count":0},{"bin_start":12.5,"bin_end":14.799999999999999,"count":0},{"bin_start":14.799999999999999,"bin_end":17.099999999999998,"count":0},{"bin_start":17.099999999999998,"bin_end":19.4,"count":0},{"bin_start":19.4,"bin_end":21.7,"count":0},{"bin_start":21.7,"bin_end":24,"count":2}]}},{"name":"TAX","dtype":"float64","stats":{"unique_count":3,"nan_count":0,"min":304,"max":666,"histogram":[{"bin_start":304,"bin_end":340.2,"count":3},{"bin_start":340.2,"bin_end":376.4,"count":0},{"bin_start":376.4,"bin_end":412.6,"count":0},{"bin_start":412.6,"bin_end":448.8,"count":0},{"bin_start":448.8,"bin_end":485,"count":0},{"bin_start":485,"bin_end":521.2,"count":0},{"bin_start":521.2,"bin_end":557.4000000000001,"count":0},{"bin_start":557.4000000000001,"bin_end":593.6,"count":0},{"bin_start":593.6,"bin_end":629.8,"count":0},{"bin_start":629.8,"bin_end":666,"count":2}]}},{"name":"PTRATIO","dtype":"float64","stats":{"unique_count":3,"nan_count":0,"min":16.9,"max":21,"histogram":[{"bin_start":16.9,"bin_end":17.31,"count":1},{"bin_start":17.31,"bin_end":17.72,"count":0},{"bin_start":17.72,"bin_end":18.13,"count":0},{"bin_start":18.13,"bin_end":18.54,"count":0},{"bin_start":18.54,"bin_end":18.95,"count":0},{"bin_start":18.95,"bin_end":19.36,"count":0},{"bin_start":19.36,"bin_end":19.77,"count":0},{"bin_start":19.77,"bin_end":20.18,"count":0},{"bin_start":20.18,"bin_end":20.59,"count":2},{"bin_start":20.59,"bin_end":21,"count":2}]}},{"name":"B","dtype":"float64","stats":{"unique_count":5,"nan_count":0,"min":2.52,"max":395.62,"histogram":[{"bin_start":2.52,"bin_end":41.830000000000005,"count":1},{"bin_start":41.830000000000005,"bin_end":81.14,"count":0},{"bin_start":81.14,"bin_end":120.45,"count":0},{"bin_start":120.45,"bin_end":159.76000000000002,"count":0},{"bin_start":159.76000000000002,"bin_end":199.07000000000002,"count":0},{"bin_start":199.07000000000002,"bin_end":238.38000000000002,"count":0},{"bin_start":238.38000000000002,"bin_end":277.69,"count":0},{"bin_start":277.69,"bin_end":317,"count":0},{"bin_start":317,"bin_end":356.31,"count":1},{"bin_start":356.31,"bin_end":395.62,"count":3}]}},{"name":"LSTAT","dtype":"float64","stats":{"unique_count":5,"nan_count":0,"min":7.83,"max":24.91,"histogram":[{"bin_start":7.83,"bin_end":9.538,"count":2},{"bin_start":9.538,"bin_end":11.245999999999999,"count":0},{"bin_start":11.245999999999999,"bin_end":12.953999999999999,"count":1},{"bin_start":12.953999999999999,"bin_end":14.661999999999999,"count":0},{"bin_start":14.661999999999999,"bin_end":16.369999999999997,"count":0},{"bin_start":16.369999999999997,"bin_end":18.077999999999996,"count":0},{"bin_start":18.077999999999996,"bin_end":19.785999999999998,"count":0},{"bin_start":19.785999999999998,"bin_end":21.494,"count":0},{"bin_start":21.494,"bin_end":23.201999999999998,"count":0},{"bin_start":23.201999999999998,"bin_end":24.91,"count":2}]}},{"name":"MEDV","dtype":"float64","stats":{"unique_count":5,"nan_count":0,"min":12,"max":19.9,"histogram":[{"bin_start":12,"bin_end":12.79,"count":1},{"bin_start":12.79,"bin_end":13.58,"count":1},{"bin_start":13.58,"bin_end":14.37,"count":0},{"bin_start":14.37,"bin_end":15.16,"count":0},{"bin_start":15.16,"bin_end":15.95,"count":0},{"bin_start":15.95,"bin_end":16.74,"count":0},{"bin_start":16.74,"bin_end":17.529999999999998,"count":0},{"bin_start":17.529999999999998,"bin_end":18.32,"count":1},{"bin_start":18.32,"bin_end":19.11,"count":0},{"bin_start":19.11,"bin_end":19.9,"count":2}]}},{"name":"_deepnote_index_column","dtype":"int64"}],"rows_top":[{"CRIM":15.0234,"ZN":0,"INDUS":18.1,"CHAS":0,"NOX":0.614,"RM":5.303999999999999,"AGE":97.3,"DIS":2.1007,"RAD":24,"TAX":666,"PTRATIO":20.2,"B":349.48,"LSTAT":24.91,"MEDV":12,"_deepnote_index_column":0},{"CRIM":0.62739,"ZN":0,"INDUS":8.14,"CHAS":0,"NOX":0.5379999999999999,"RM":5.834,"AGE":56.5,"DIS":4.4986,"RAD":4,"TAX":307,"PTRATIO":21,"B":395.62,"LSTAT":8.47,"MEDV":19.9,"_deepnote_index_column":1},{"CRIM":0.03466,"ZN":35,"INDUS":6.06,"CHAS":0,"NOX":0.4379,"RM":6.031000000000001,"AGE":23.3,"DIS":6.6407,"RAD":1,"TAX":304,"PTRATIO":16.9,"B":362.25,"LSTAT":7.83,"MEDV":19.4,"_deepnote_index_column":2},{"CRIM":7.05042,"ZN":0,"INDUS":18.1,"CHAS":0,"NOX":0.614,"RM":6.103,"AGE":85.1,"DIS":2.0218,"RAD":24,"TAX":666,"PTRATIO":20.2,"B":2.52,"LSTAT":23.29,"MEDV":13.4,"_deepnote_index_column":3},{"CRIM":0.7258,"ZN":0,"INDUS":8.14,"CHAS":0,"NOX":0.5379999999999999,"RM":5.727,"AGE":69.5,"DIS":3.7965,"RAD":4,"TAX":307,"PTRATIO":21,"B":390.95,"LSTAT":11.28,"MEDV":18.2,"_deepnote_index_column":4}],"rows_bottom":null},"text/plain":"       CRIM    ZN  INDUS  CHAS     NOX     RM   AGE     DIS   RAD    TAX  \\\n0  15.02340   0.0  18.10   0.0  0.6140  5.304  97.3  2.1007  24.0  666.0   \n1   0.62739   0.0   8.14   0.0  0.5380  5.834  56.5  4.4986   4.0  307.0   \n2   0.03466  35.0   6.06   0.0  0.4379  6.031  23.3  6.6407   1.0  304.0   \n3   7.05042   0.0  18.10   0.0  0.6140  6.103  85.1  2.0218  24.0  666.0   \n4   0.72580   0.0   8.14   0.0  0.5380  5.727  69.5  3.7965   4.0  307.0   \n\n   PTRATIO       B  LSTAT  MEDV  \n0     20.2  349.48  24.91  12.0  \n1     21.0  395.62   8.47  19.9  \n2     16.9  362.25   7.83  19.4  \n3     20.2    2.52  23.29  13.4  \n4     21.0  390.95  11.28  18.2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CRIM</th>\n      <th>ZN</th>\n      <th>INDUS</th>\n      <th>CHAS</th>\n      <th>NOX</th>\n      <th>RM</th>\n      <th>AGE</th>\n      <th>DIS</th>\n      <th>RAD</th>\n      <th>TAX</th>\n      <th>PTRATIO</th>\n      <th>B</th>\n      <th>LSTAT</th>\n      <th>MEDV</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>15.02340</td>\n      <td>0.0</td>\n      <td>18.10</td>\n      <td>0.0</td>\n      <td>0.6140</td>\n      <td>5.304</td>\n      <td>97.3</td>\n      <td>2.1007</td>\n      <td>24.0</td>\n      <td>666.0</td>\n      <td>20.2</td>\n      <td>349.48</td>\n      <td>24.91</td>\n      <td>12.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.62739</td>\n      <td>0.0</td>\n      <td>8.14</td>\n      <td>0.0</td>\n      <td>0.5380</td>\n      <td>5.834</td>\n      <td>56.5</td>\n      <td>4.4986</td>\n      <td>4.0</td>\n      <td>307.0</td>\n      <td>21.0</td>\n      <td>395.62</td>\n      <td>8.47</td>\n      <td>19.9</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.03466</td>\n      <td>35.0</td>\n      <td>6.06</td>\n      <td>0.0</td>\n      <td>0.4379</td>\n      <td>6.031</td>\n      <td>23.3</td>\n      <td>6.6407</td>\n      <td>1.0</td>\n      <td>304.0</td>\n      <td>16.9</td>\n      <td>362.25</td>\n      <td>7.83</td>\n      <td>19.4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7.05042</td>\n      <td>0.0</td>\n      <td>18.10</td>\n      <td>0.0</td>\n      <td>0.6140</td>\n      <td>6.103</td>\n      <td>85.1</td>\n      <td>2.0218</td>\n      <td>24.0</td>\n      <td>666.0</td>\n      <td>20.2</td>\n      <td>2.52</td>\n      <td>23.29</td>\n      <td>13.4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.72580</td>\n      <td>0.0</td>\n      <td>8.14</td>\n      <td>0.0</td>\n      <td>0.5380</td>\n      <td>5.727</td>\n      <td>69.5</td>\n      <td>3.7965</td>\n      <td>4.0</td>\n      <td>307.0</td>\n      <td>21.0</td>\n      <td>390.95</td>\n      <td>11.28</td>\n      <td>18.2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Separating Input Features and Output Features\nBefore building any machine learning model, we always separate the input variables and output variables. \n\n**Input Variables or Independent Variables** are those quantities whose values are changed naturally in an experiment\n\n**Output Variable or Dependent Variable** is the one whose values are dependent on the input variables. \n\nLike here in this data, we are trying to predict the price of a houce i.e. 'MEDV', so this is our Output Variable \n\nBy convention input variables are represented with 'X' and output variables are represented with 'y'.","metadata":{"id":"DBv1fy29jAzm","colab_type":"text","cell_id":"00011-02c64bb5-74f1-4640-b63b-4ed8943b0a0a"}},{"cell_type":"code","metadata":{"id":"ErMptop0jyng","colab_type":"code","colab":{},"cell_id":"00012-a51250fe-1725-46bd-94f6-d7e42397f302"},"source":"X = boston_data.drop('MEDV', axis = 1)    # Input Variables/features\ny = boston_data.MEDV      # output variables/features","execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Splitting the data","metadata":{"id":"QgXPagiS3YKz","colab_type":"text","cell_id":"00013-5ebdbdb8-1f1f-4a82-ad14-6e3fc492270f"}},{"cell_type":"markdown","source":"We want to check the performance of the model that we built. For this purpose, we always split the given data(both input and output data)  into **training set** which will be used to train the model, and **test set** which will be used to check how accurately the model is predicting outcomes.\n\nFor this purpose we have a class called 'train_test_split' in the 'sklearn.model_selection' module.\n\nWe split 80% of the data to the training set while 20% of the data to test set using below code.\nThe test_size variable is where we actually specify the proportion of the test set.\n\nBy passing our X and y variables into the train_test_split method, we are able to capture the splits in data by assigning 4 variables to the result.","metadata":{"id":"4vOkLlB-Ukjn","colab_type":"text","cell_id":"00014-3182098d-aff3-48b7-8eaa-6bada01bb54f"}},{"cell_type":"code","metadata":{"id":"1a1OFJmpUl2u","colab_type":"code","colab":{},"cell_id":"00015-a67d0e9f-1909-440b-80a3-01562458a1cf"},"source":"# import train_test_split\nfrom sklearn.model_selection import train_test_split \n\n# Assign variables to capture train test split output\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# X_train: independent/input feature data for training the model\n# y_train: dependent/output feature data for training the model\n# X_test: independent/input feature data for testing the model; will be used to predict the output values\n# y_test: original dependent/output values of X_test; We will compare this values with our predicted values to check the performance of our built model.\n \n# test_size = 0.20: 20% of the data will go for test set and 70% of the data will go for train set\n# random_state = 42: this will fix the split i.e. there will be same split for each time you run the code","execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"UfKNjpJqkPUf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"a1f5d8fc-98e1-405b-9ed4-24f44a57660f","cell_id":"00016-a86faef3-c922-49df-b31a-1448f9b54ea3"},"source":"# find the number of input features\nn_features = X.shape[1]\nprint(n_features)","execution_count":7,"outputs":[{"name":"stdout","text":"13\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Training our model\n","metadata":{"id":"CwotIN0AU3Ci","colab_type":"text","cell_id":"00017-1f8cd2b7-7e6d-4d88-b8b2-83838b77969d"}},{"cell_type":"markdown","source":"After splitting the data into training and testing sets, it's time to train our first deep learning model. Wait! Before training the deep learning model, let's understand the **Deep Learning Model Life-Cycle**.","metadata":{"id":"5EV_0iiBMBz6","colab_type":"text","cell_id":"00018-7124c441-4edd-48fa-83d4-a1fb85d3adcf"}},{"cell_type":"markdown","source":"## Neural Network: Architecture\nHere we are giving you just an overview of the architecture of Neural Network. You will know more about it in future modules.\n\nNeural Networks consists of an input and output layer with one or more hidden layers.\n\n![neural network architecture](https://dphi-courses.s3.ap-south-1.amazonaws.com/Deep+Learning+Bootcamp/nn+arch.png)","metadata":{"id":"j3XXW9NL6H4s","colab_type":"text","cell_id":"00019-0a5878e3-a7fa-4d92-9f7f-9481832a2a76"}},{"cell_type":"markdown","source":"## The 5 Step Model Life-Cycle\n\nA model has a life-cycle, and this very simple knowledge provides the backbone for both modeling a dataset and understanding the tf.keras API.\n\nThe five steps in the life-cycle are as follows:\n\n1. Define the model.\n2. Compile the model.\n3. Fit the model.\n4. Make predictions on the test data.\n5. Evaluate the model.\n\nWe will take closer look into each of the steps and parallely build the deep learning model. Also, don't worry about the code for the moment if you don't understand it here. For the time being focus on the flow of building a deep learning model.","metadata":{"id":"Ymbh14OgMu2Z","colab_type":"text","cell_id":"00020-02a6f4cb-bf11-464e-ae45-4bcba2ae2139"}},{"cell_type":"markdown","source":"### 1. Define the model\nDefining the model requires that you first select the type of model that you need and then choose the architecture or network topology.\n\nModels can be defined either with the Sequential API or the Functional API (you will know this in later modules). Here we will define the model with Sequential API. Now **what is Sequential API?**\n\n**Sequential API**\nThe sequential API is the simplest API to get started with Deep Learning. \nYou will know more about it in upcoming learning units.\n\nThe sequential API allows you to **create models layer-by-layer**\n\n**What is layers?**\n\nA layer groups a number of neurons together. It is used for holding a collection of neurons. The **learning process** of a neural network is performed with the layers. The key to note is that the neurons are placed within layers and each layer has its purpose.\n\n","metadata":{"id":"auQ5lPFrN7GN","colab_type":"text","cell_id":"00021-aabe70e2-8cfb-4249-bda9-bbba23b2ffad"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00022-666fa846-9891-4db3-a90b-4eb6945fc50a"},"source":"pip install tensorflow","execution_count":8,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow in /opt/venv/lib/python3.7/site-packages (2.3.0)\nRequirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /opt/venv/lib/python3.7/site-packages (from tensorflow) (1.1.2)\nRequirement already satisfied: wrapt>=1.11.1 in /opt/venv/lib/python3.7/site-packages (from tensorflow) (1.12.1)\nRequirement already satisfied: absl-py>=0.7.0 in /opt/venv/lib/python3.7/site-packages (from tensorflow) (0.10.0)\nRequirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /opt/venv/lib/python3.7/site-packages (from tensorflow) (2.3.0)\nRequirement already satisfied: google-pasta>=0.1.8 in /opt/venv/lib/python3.7/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: six>=1.12.0 in /opt/venv/lib/python3.7/site-packages (from tensorflow) (1.15.0)\nRequirement already satisfied: protobuf>=3.9.2 in /opt/venv/lib/python3.7/site-packages (from tensorflow) (3.13.0)\nRequirement already satisfied: astunparse==1.6.3 in /opt/venv/lib/python3.7/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: scipy==1.4.1 in /opt/venv/lib/python3.7/site-packages (from tensorflow) (1.4.1)\nRequirement already satisfied: tensorboard<3,>=2.3.0 in /opt/venv/lib/python3.7/site-packages (from tensorflow) (2.3.0)\nRequirement already satisfied: grpcio>=1.8.6 in /opt/venv/lib/python3.7/site-packages (from tensorflow) (1.31.0)\nRequirement already satisfied: wheel>=0.26 in /opt/venv/lib/python3.7/site-packages (from tensorflow) (0.34.2)\nRequirement already satisfied: gast==0.3.3 in /opt/venv/lib/python3.7/site-packages (from tensorflow) (0.3.3)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/venv/lib/python3.7/site-packages (from tensorflow) (1.1.0)\nRequirement already satisfied: h5py<2.11.0,>=2.10.0 in /opt/venv/lib/python3.7/site-packages (from tensorflow) (2.10.0)\nRequirement already satisfied: numpy<1.19.0,>=1.16.0 in /opt/venv/lib/python3.7/site-packages (from tensorflow) (1.18.5)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/venv/lib/python3.7/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: setuptools in /opt/venv/lib/python3.7/site-packages (from protobuf>=3.9.2->tensorflow) (47.3.1)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/venv/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/venv/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.1)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/venv/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.24.0)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/venv/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/venv/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.20.1)\nRequirement already satisfied: markdown>=2.6.8 in /opt/venv/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.2.2)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/venv/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/venv/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\nRequirement already satisfied: idna<3,>=2.5 in /opt/venv/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/venv/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.25.9)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/venv/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/venv/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/venv/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\nRequirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in /opt/venv/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\nRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/venv/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/venv/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/venv/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\nRequirement already satisfied: zipp>=0.5 in /opt/venv/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.2 is available.\nYou should consider upgrading via the '/opt/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"id":"JY303RWVPzkD","colab_type":"code","colab":{},"cell_id":"00022-6e6ff950-d3cc-44c5-b24e-537248e1e08e"},"source":"from tensorflow.keras import Sequential    # import Sequential from tensorflow.keras\nfrom tensorflow.keras.layers import Dense  # import Dense from tensorflow.keras.layers\nfrom numpy.random import seed     # seed helps you to fix the randomness in the neural network.  \nimport tensorflow","execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"mMDrQE9gQP6U","colab_type":"code","colab":{},"cell_id":"00023-edab2bfb-3caa-4167-8f20-131080f14007"},"source":"# define the model\nmodel = Sequential()\nmodel.add(Dense(10, activation='relu', input_shape=(n_features,)))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dense(1))","execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Note that the visible layer of the network is defined by the “input_shape” argument on the first hidden layer. \n\nThe sequential API is easy to use because you keep calling model.add() until you have added all of your layers.","metadata":{"id":"93ntHsm1QnLP","colab_type":"text","cell_id":"00024-25c65463-bd24-4863-8e34-5cfa9a954ca7"}},{"cell_type":"markdown","source":"The activation function we have chosen is **ReLU**, which stands for **rectified linear unit**. Activation function decides, whether a neuron should be activated or not\n\nReLU is an activation function which is defined mathematically as **F(x) = max(0,x)**. In other words, the output is x, if x is greater than 0, and the output is 0 if x is 0 or negative.","metadata":{"id":"ABBhVYtXRlJq","colab_type":"text","cell_id":"00025-68359248-464b-41cb-b2c7-2d9bf47592f8"}},{"cell_type":"markdown","source":"### 2. Compile the model\nCompiling the model requires that you first select a loss function that you want to optimize, such as mean squared error or cross-entropy.\n\nIt also requires that you select an algorithm to perform the optimization procedure. We’re using **RMSprop** as our optimizer here. R \n\nIt may also require that you select any performance metrics to keep track of during the model training process. The loss function used here is **mean squared error.**\n\nFrom an API perspective, this involves calling a function to compile the model with the chosen configuration, which will prepare the appropriate data structures required for the efficient use of the model you have defined.","metadata":{"id":"D100gBMQSTn8","colab_type":"text","cell_id":"00026-88086d8c-d802-4df4-aa71-4c4905f8676e"}},{"cell_type":"code","metadata":{"id":"doYNy1jJQa7J","colab_type":"code","colab":{},"cell_id":"00027-f54ab063-b86f-42ca-8879-ce8d24188c97"},"source":"# import RMSprop optimizer\nfrom tensorflow.keras.optimizers import RMSprop\noptimizer = RMSprop(0.01)    # 0.01 is the learning rate","execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"**Why learning rate = 0.01?**\n\nIt is important to find a good value for the learning rate for your model on your training dataset. we cannot analytically calculate the optimal learning rate for a given model on a given dataset. Instead, a good (or good enough) learning rate must be discovered via trial and error.\n\nThe range of values to consider for the learning rate is less than 1.0 and greater than $10^{-6}$.\n\nA traditional default value for the learning rate is 0.1 or 0.01, and this may represent a good starting point on your problem.","metadata":{"id":"AiOzHe6K8iaM","colab_type":"text","cell_id":"00028-824f4fc5-033d-478c-84b5-e4e54c5eef67"}},{"cell_type":"code","metadata":{"id":"gNxnRf2uUqev","colab_type":"code","colab":{},"cell_id":"00029-3af864dc-e3f8-4485-9391-b5254352f255"},"source":"model.compile(loss='mean_squared_error',optimizer=optimizer)    # compile the model","execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### 3. Fitting the model\nFitting the model requires that you first select the training configuration, such as the number of epochs (loops through the training dataset) and the batch size (number of samples in an epoch used to estimate model error).\n\nFrom an API perspective, this involves calling a function to perform the training process. This function will block (not return) until the training process has finished.\n","metadata":{"id":"GQPPF22pVHMU","colab_type":"text","cell_id":"00030-a66ec8c6-0964-4bf7-93c6-f4eb1eb09562"}},{"cell_type":"code","metadata":{"id":"WnG7pev1UxNs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":390},"outputId":"ebce7a5c-31c9-400c-a1d7-70e6c26bec64","cell_id":"00031-10f7b025-4ed7-4d5e-aa09-95421d400f6f"},"source":"seed_value = 42\nseed(seed_value)        # If you build the model with given parameters, set_random_seed will help you produce the same result on multiple execution\n\n\n# Recommended by Keras -------------------------------------------------------------------------------------\n# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\nimport os\nos.environ['PYTHONHASHSEED']=str(seed_value)\n\n# 2. Set `python` built-in pseudo-random generator at a fixed value\nimport random\nrandom.seed(seed_value)\n\n# 3. Set `numpy` pseudo-random generator at a fixed value\nimport numpy as np\nnp.random.seed(seed_value)\n# Recommended by Keras -------------------------------------------------------------------------------------\n\n\n# 4. Set the `tensorflow` pseudo-random generator at a fixed value\ntensorflow.random.set_seed(seed_value) \nmodel.fit(X_train, y_train, epochs=10, batch_size=30, verbose = 1)    # fit the model","execution_count":13,"outputs":[{"name":"stdout","text":"Epoch 1/10\n11/11 [==============================] - 0s 3ms/step - loss: 1375.7964\nEpoch 2/10\n11/11 [==============================] - 0s 2ms/step - loss: 130.9068\nEpoch 3/10\n11/11 [==============================] - 0s 2ms/step - loss: 570.0474\nEpoch 4/10\n11/11 [==============================] - 0s 2ms/step - loss: 370.5878\nEpoch 5/10\n11/11 [==============================] - 0s 2ms/step - loss: 225.8166\nEpoch 6/10\n11/11 [==============================] - 0s 2ms/step - loss: 297.5669\nEpoch 7/10\n11/11 [==============================] - 0s 2ms/step - loss: 98.5439\nEpoch 8/10\n11/11 [==============================] - 0s 2ms/step - loss: 259.3184\nEpoch 9/10\n11/11 [==============================] - 0s 2ms/step - loss: 116.9864\nEpoch 10/10\n11/11 [==============================] - 0s 2ms/step - loss: 138.1832\n","output_type":"stream"},{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7fbae03d79e8>"},"metadata":{}}]},{"cell_type":"markdown","source":"What is **verbose**?\n\nBy setting verbose 0, 1 or 2 you just say how do you want to 'see' the training progress for each epoch.\n\n`verbose=0` will show you nothing (silent)\n\n`verbose=1` will show you an animated progress bar like this:\n\n![progres_bar](https://dphi-courses.s3.ap-south-1.amazonaws.com/Deep+Learning+Bootcamp/progress+bar.png)\n\n`verbose=2` will just mention the number of epoch like this:\n\n![verbose = 2](https://dphi-courses.s3.ap-south-1.amazonaws.com/Deep+Learning+Bootcamp/epoch.png)","metadata":{"id":"XZzI8-Kl_Rdg","colab_type":"text","cell_id":"00032-d96f15f1-6108-4b37-934e-1820f6588612"}},{"cell_type":"markdown","source":"### 4. Evaluate the model\nEvaluating the model requires that you first choose a holdout dataset used to evaluate the model. This should be data not used in the training process i.e. the X_test.\n\nFrom an API perspective, this involves calling a function with the holdout dataset and getting a loss and perhaps other metrics that can be reported.","metadata":{"id":"t_tKcVVzhJf8","colab_type":"text","cell_id":"00033-6123d1de-6eea-4db3-ab9e-a8993be70ea0"}},{"cell_type":"code","metadata":{"id":"qC63QoqchIa1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"685a24bb-e955-4570-fa15-70b7f0996d11","cell_id":"00034-c01de4fd-3a93-4dda-8aaf-1ee8d182680e"},"source":"model.evaluate(X_test, y_test)","execution_count":14,"outputs":[{"name":"stdout","text":"3/3 [==============================] - 0s 1ms/step - loss: 122.9007\n","output_type":"stream"},{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"122.90074157714844"},"metadata":{}}]},{"cell_type":"markdown","source":"The mean squared error we got here is 64.8. Now, **what does it mean?**\n\nWhen you subtract the predicted values (of X_test data) from the acutal value (of X_test data), then square it and sum all the squares, and finally take a mean (i.e. average) of it, the result you will get is 64.8 in this case.\n\nevaluate() does this task automatically. If you want to get the prediciton for X_test you can do **`model.predict(X_test)`**","metadata":{"id":"gHM4sYEOnD7j","colab_type":"text","cell_id":"00035-9fffbd2d-fc3a-4d70-9cdc-a36777202164"}},{"cell_type":"markdown","source":"#### Hyperparameter Tunning\nThe hyperparameters here in this notebook are:\n1. Learning Rate\n2. Epochs\n3. Batch Size\n\nWe can try and change the values of these parameters and see the performance  of the model (evaluate the model) on X_test data","metadata":{"id":"li94FIlfnMBB","colab_type":"text","cell_id":"00036-c6c0601f-5464-4489-993f-9b29b7c2e134"}},{"cell_type":"markdown","source":"**Learning Rate**\n\nA scalar used to train a model via gradient descent. During each iteration, the **gradient descent** algorithm multiplies the learning rate by the gradient. The resulting product is called the **gradient step**.\n\nLearning rate is a key **hyperparameter**.","metadata":{"id":"XljOzViRoRbp","colab_type":"text","cell_id":"00037-ec695776-6533-4a65-aed8-78134db4799f"}},{"cell_type":"code","metadata":{"id":"puhKnL9KKuZX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":407},"outputId":"2ee8323d-c6e3-414a-9d9b-6b2ac7ca641f","cell_id":"00038-26f3d064-7fa8-41d4-a66d-781fc2a330b9"},"source":"####################### Complete example to check the performance of the model with different learning rates #######################################\n# define the model\nmodel = Sequential()\nmodel.add(Dense(10, activation='relu', input_shape=(n_features,)))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dense(1))\n\noptimizer = RMSprop(0.1)    # 0.1 is the learning rate\nmodel.compile(loss='mean_squared_error',optimizer=optimizer)    # compile the model\n\n# fit the model \nmodel.fit(X_train, y_train, epochs=10, batch_size=30, verbose = 1)\n\n# evaluate the model\nprint('The MSE value is: ', model.evaluate(X_test, y_test))","execution_count":15,"outputs":[{"name":"stdout","text":"Epoch 1/10\n11/11 [==============================] - 0s 1ms/step - loss: 35268.1797\nEpoch 2/10\n11/11 [==============================] - 0s 1ms/step - loss: 559.4070\nEpoch 3/10\n11/11 [==============================] - 0s 1ms/step - loss: 212.1510\nEpoch 4/10\n11/11 [==============================] - 0s 1ms/step - loss: 139.3775\nEpoch 5/10\n11/11 [==============================] - 0s 1ms/step - loss: 129.5609\nEpoch 6/10\n11/11 [==============================] - 0s 1ms/step - loss: 122.8495\nEpoch 7/10\n11/11 [==============================] - 0s 1ms/step - loss: 174.4107\nEpoch 8/10\n11/11 [==============================] - 0s 1ms/step - loss: 142.8209\nEpoch 9/10\n11/11 [==============================] - 0s 1ms/step - loss: 201.2050\nEpoch 10/10\n11/11 [==============================] - 0s 2ms/step - loss: 223.9926\n3/3 [==============================] - 0s 2ms/step - loss: 122.8342\nThe MSE value is:  122.834228515625\n","output_type":"stream"}]},{"cell_type":"markdown","source":"As you can see above, how the loss (cost) i.e. MSE has changed by just changing the learning rate.","metadata":{"id":"VPneBHxHYxfC","colab_type":"text","cell_id":"00039-022be74c-9b33-490a-9663-03fc062f06f0"}},{"cell_type":"markdown","source":"### Exercise 1\n\nTest several learning rate values to see the impact of varying this value when defining your model.","metadata":{"id":"fhi50q4rhUiF","colab_type":"text","cell_id":"00040-69867fd6-3791-472c-9f8b-74ffb17f3a25"}},{"cell_type":"code","metadata":{"id":"k4BqC2Cfo159","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":407},"outputId":"101ef07c-eadb-44a9-df34-f02b0c285dd5","cell_id":"00041-8e5e558c-2c5f-455a-af73-700c2ce7f776"},"source":"# Play with learning rate\nlearning_rate = 0.05          # Replace ? with a floating-point number(decimal no.)\nepochs = 10\noptimizer = RMSprop(learning_rate)\nmodel.compile(loss='mean_squared_error',optimizer=optimizer)    # Compile the model\nmodel.fit(X_train, y_train, epochs=epochs, batch_size=30)       # Fit the model\nmodel.evaluate(X_test, y_test)                                  # Evaluate the model","execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/10\n11/11 [==============================] - 0s 2ms/step - loss: 195.4908\nEpoch 2/10\n11/11 [==============================] - 0s 2ms/step - loss: 95.2293\nEpoch 3/10\n11/11 [==============================] - 0s 2ms/step - loss: 91.5189\nEpoch 4/10\n11/11 [==============================] - 0s 2ms/step - loss: 91.8507\nEpoch 5/10\n11/11 [==============================] - 0s 2ms/step - loss: 90.9590\nEpoch 6/10\n11/11 [==============================] - 0s 2ms/step - loss: 91.7559\nEpoch 7/10\n11/11 [==============================] - 0s 2ms/step - loss: 91.7865\nEpoch 8/10\n11/11 [==============================] - 0s 2ms/step - loss: 91.6551\nEpoch 9/10\n11/11 [==============================] - 0s 2ms/step - loss: 92.1092\nEpoch 10/10\n11/11 [==============================] - 0s 2ms/step - loss: 91.6490\n3/3 [==============================] - 0s 3ms/step - loss: 71.1137\n","output_type":"stream"},{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"71.1137466430664"},"metadata":{}}]},{"cell_type":"markdown","source":"Learning rate of 0.05 gave a better loss than 0.01 and 0.1","metadata":{"id":"sZZz6FFT-eg5","colab_type":"text","cell_id":"00042-c4688b0a-bcdb-4753-b8a6-5792cf48a396"}},{"cell_type":"code","metadata":{"id":"hEm6IIYV-b99","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":407},"outputId":"5d717c9a-8097-47fa-f2cf-68d9f021d6b6","cell_id":"00043-cfdd5b45-81a7-4919-933b-6bcbb2c45997"},"source":"# Play with learning rate\nlearning_rate = 0.07          # Replace ? with a floating-point number(decimal no.)\nepochs = 10\noptimizer = RMSprop(learning_rate)\nmodel.compile(loss='mean_squared_error',optimizer=optimizer)    # Compile the model\nmodel.fit(X_train, y_train, epochs=epochs, batch_size=30)       # Fit the model\nmodel.evaluate(X_test, y_test)     ","execution_count":null,"outputs":[{"output_type":"stream","text":"Epoch 1/10\n11/11 [==============================] - 0s 1ms/step - loss: 93.2686\nEpoch 2/10\n11/11 [==============================] - 0s 1ms/step - loss: 92.3033\nEpoch 3/10\n11/11 [==============================] - 0s 1ms/step - loss: 92.3514\nEpoch 4/10\n11/11 [==============================] - 0s 1ms/step - loss: 93.5292\nEpoch 5/10\n11/11 [==============================] - 0s 1ms/step - loss: 90.8375\nEpoch 6/10\n11/11 [==============================] - 0s 1ms/step - loss: 92.8818\nEpoch 7/10\n11/11 [==============================] - 0s 1ms/step - loss: 92.0054\nEpoch 8/10\n11/11 [==============================] - 0s 1ms/step - loss: 91.9995\nEpoch 9/10\n11/11 [==============================] - 0s 1ms/step - loss: 92.5875\nEpoch 10/10\n11/11 [==============================] - 0s 1ms/step - loss: 91.9638\n3/3 [==============================] - 0s 2ms/step - loss: 71.2276\n","name":"stdout"},{"output_type":"execute_result","data":{"text/plain":"71.2276382446289"},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","source":"No improvement in Loss function even with increase in learning rate. Learning rate reduces","metadata":{"id":"gX2BRhM3-rAm","colab_type":"text","cell_id":"00044-a2cf95b4-13bc-489f-b145-59d728fb673c"}},{"cell_type":"code","metadata":{"id":"p76bc6Qz-_ll","colab_type":"code","colab":{},"cell_id":"00045-a603b768-7bc2-47e3-a572-57eda1574cc9"},"source":"# Play with learning rate\nlearning_rate = 0.03          # Replace ? with a floating-point number(decimal no.)\nepochs = 10\noptimizer = RMSprop(learning_rate)\nmodel.compile(loss='mean_squared_error',optimizer=optimizer)    # Compile the model\nmodel.fit(X_train, y_train, epochs=epochs, batch_size=30)       # Fit the model\nmodel.evaluate(X_test, y_test)     ","execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/10\n11/11 [==============================] - 0s 2ms/step - loss: 92.7415\nEpoch 2/10\n11/11 [==============================] - 0s 1ms/step - loss: 91.4797\nEpoch 3/10\n11/11 [==============================] - 0s 1ms/step - loss: 91.3850\nEpoch 4/10\n11/11 [==============================] - 0s 1ms/step - loss: 91.9587\nEpoch 5/10\n11/11 [==============================] - 0s 2ms/step - loss: 90.9638\nEpoch 6/10\n11/11 [==============================] - 0s 1ms/step - loss: 91.5577\nEpoch 7/10\n11/11 [==============================] - 0s 1ms/step - loss: 91.5831\nEpoch 8/10\n11/11 [==============================] - 0s 1ms/step - loss: 91.4145\nEpoch 9/10\n11/11 [==============================] - 0s 1ms/step - loss: 91.6374\nEpoch 10/10\n11/11 [==============================] - 0s 1ms/step - loss: 91.3478\n3/3 [==============================] - 0s 3ms/step - loss: 71.0999\n","output_type":"stream"},{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"71.09992980957031"},"metadata":{}}]},{"cell_type":"markdown","source":"###### Learning rate at 0.03 shows an improvement from previous ones tries","metadata":{"tags":[],"cell_id":"00047-7e2904ea-23f2-41f2-a6b6-30ace49c94f2"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00047-de3cbc78-0873-4770-a9cd-2c96aa7d24b7"},"source":"# Play with learning rate\nlearning_rate = 0.02          # Replace ? with a floating-point number(decimal no.)\nepochs = 10\noptimizer = RMSprop(learning_rate)\nmodel.compile(loss='mean_squared_error',optimizer=optimizer)    # Compile the model\nmodel.fit(X_train, y_train, epochs=epochs, batch_size=30)       # Fit the model\nmodel.evaluate(X_test, y_test)     ","execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/10\n11/11 [==============================] - 0s 2ms/step - loss: 92.2870\nEpoch 2/10\n11/11 [==============================] - 0s 2ms/step - loss: 91.2810\nEpoch 3/10\n11/11 [==============================] - 0s 3ms/step - loss: 91.1849\nEpoch 4/10\n11/11 [==============================] - 0s 2ms/step - loss: 91.5739\nEpoch 5/10\n11/11 [==============================] - 0s 2ms/step - loss: 90.9561\nEpoch 6/10\n11/11 [==============================] - 0s 2ms/step - loss: 91.2879\nEpoch 7/10\n11/11 [==============================] - 0s 2ms/step - loss: 91.3818\nEpoch 8/10\n11/11 [==============================] - 0s 2ms/step - loss: 91.2628\nEpoch 9/10\n11/11 [==============================] - 0s 2ms/step - loss: 91.3745\nEpoch 10/10\n11/11 [==============================] - 0s 2ms/step - loss: 91.1917\nWARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fabcd5e9158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n3/3 [==============================] - 0s 2ms/step - loss: 71.1003\n","output_type":"stream"},{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"71.1003189086914"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00049-7dd870cc-1c9e-4e05-8b00-32423328d02d"},"source":"# Play with learning rate\nlearning_rate = 0.01          # Replace ? with a floating-point number(decimal no.)\nepochs = 10\noptimizer = RMSprop(learning_rate)\nmodel.compile(loss='mean_squared_error',optimizer=optimizer)    # Compile the model\nmodel.fit(X_train, y_train, epochs=epochs, batch_size=30)       # Fit the model\nmodel.evaluate(X_test, y_test)     ","execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/10\n11/11 [==============================] - 0s 1ms/step - loss: 91.7984\nEpoch 2/10\n11/11 [==============================] - 0s 1ms/step - loss: 91.1087\nEpoch 3/10\n11/11 [==============================] - 0s 2ms/step - loss: 91.0039\nEpoch 4/10\n11/11 [==============================] - 0s 2ms/step - loss: 91.1941\nEpoch 5/10\n11/11 [==============================] - 0s 1ms/step - loss: 90.9154\nEpoch 6/10\n11/11 [==============================] - 0s 1ms/step - loss: 91.0506\nEpoch 7/10\n11/11 [==============================] - 0s 1ms/step - loss: 91.1293\nEpoch 8/10\n11/11 [==============================] - 0s 1ms/step - loss: 91.0805\nEpoch 9/10\n11/11 [==============================] - 0s 1ms/step - loss: 91.1054\nEpoch 10/10\n11/11 [==============================] - 0s 1ms/step - loss: 91.0223\nWARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fabce751e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n3/3 [==============================] - 0s 2ms/step - loss: 71.1032\n","output_type":"stream"},{"output_type":"execute_result","execution_count":20,"data":{"text/plain":"71.10315704345703"},"metadata":{}}]},{"cell_type":"markdown","source":"## The optimum learning rate is 0.03","metadata":{"tags":[],"cell_id":"00050-b3fbbe93-6f90-4036-8aed-eb546ee16e30"}},{"cell_type":"markdown","source":"**Epochs**\n\nA full training pass over the entire dataset such that each example has been seen once. Thus, an epoch represents N/batch size training iterations, where N is the total number of examples.","metadata":{"id":"Mov9-uVwpmkd","colab_type":"text","cell_id":"00046-ee18436b-c461-4741-9087-45828e81be6b"}},{"cell_type":"code","metadata":{"id":"zlZv50iKZPqo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"36ef904c-7b07-4f63-954f-98b1a20b55f1","cell_id":"00047-99350b4f-a7d4-4d1d-baf7-b0e32ccb4531"},"source":"####################### Complete example to check the performance of the model with different epochs and learning rate = 0.01 #######################################\n# define the model\nmodel = Sequential()\nmodel.add(Dense(10, activation='relu', input_shape=(n_features,)))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dense(1))\n\noptimizer = RMSprop(0.1)    # 0.1 is the learning rate\nmodel.compile(loss='mean_squared_error',optimizer=optimizer)    # Compile the model\n\n# fit the model \nmodel.fit(X_train, y_train, epochs=100, batch_size=30, verbose = 1)\n\n# evaluate the model\nprint('The MSE value is: ', model.evaluate(X_test, y_test))","execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/100\n11/11 [==============================] - 0s 2ms/step - loss: 9724.5742\nEpoch 2/100\n11/11 [==============================] - 0s 2ms/step - loss: 582.0069\nEpoch 3/100\n11/11 [==============================] - 0s 3ms/step - loss: 555.8216\nEpoch 4/100\n11/11 [==============================] - 0s 3ms/step - loss: 521.9396\nEpoch 5/100\n11/11 [==============================] - 0s 3ms/step - loss: 483.8689\nEpoch 6/100\n11/11 [==============================] - 0s 2ms/step - loss: 444.7491\nEpoch 7/100\n11/11 [==============================] - 0s 2ms/step - loss: 406.9054\nEpoch 8/100\n11/11 [==============================] - 0s 2ms/step - loss: 371.2612\nEpoch 9/100\n11/11 [==============================] - 0s 2ms/step - loss: 337.6587\nEpoch 10/100\n11/11 [==============================] - 0s 2ms/step - loss: 306.2268\nEpoch 11/100\n11/11 [==============================] - 0s 2ms/step - loss: 277.0800\nEpoch 12/100\n11/11 [==============================] - 0s 2ms/step - loss: 250.4167\nEpoch 13/100\n11/11 [==============================] - 0s 3ms/step - loss: 225.5309\nEpoch 14/100\n11/11 [==============================] - 0s 2ms/step - loss: 202.9859\nEpoch 15/100\n11/11 [==============================] - 0s 2ms/step - loss: 182.7787\nEpoch 16/100\n11/11 [==============================] - 0s 3ms/step - loss: 164.8227\nEpoch 17/100\n11/11 [==============================] - 0s 2ms/step - loss: 148.9729\nEpoch 18/100\n11/11 [==============================] - 0s 2ms/step - loss: 135.4988\nEpoch 19/100\n11/11 [==============================] - 0s 3ms/step - loss: 123.8988\nEpoch 20/100\n11/11 [==============================] - 0s 2ms/step - loss: 114.1056\nEpoch 21/100\n11/11 [==============================] - 0s 3ms/step - loss: 106.0993\nEpoch 22/100\n11/11 [==============================] - 0s 2ms/step - loss: 100.3444\nEpoch 23/100\n11/11 [==============================] - 0s 3ms/step - loss: 96.4450\nEpoch 24/100\n11/11 [==============================] - 0s 2ms/step - loss: 93.5845\nEpoch 25/100\n11/11 [==============================] - 0s 2ms/step - loss: 92.0008\nEpoch 26/100\n11/11 [==============================] - 0s 2ms/step - loss: 91.3616\nEpoch 27/100\n11/11 [==============================] - 0s 2ms/step - loss: 91.0118\nEpoch 28/100\n11/11 [==============================] - 0s 1ms/step - loss: 91.1522\nEpoch 29/100\n11/11 [==============================] - 0s 2ms/step - loss: 91.0371\nEpoch 30/100\n11/11 [==============================] - 0s 2ms/step - loss: 91.0573\nEpoch 31/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.9193\nEpoch 32/100\n11/11 [==============================] - 0s 2ms/step - loss: 91.0018\nEpoch 33/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.9787\nEpoch 34/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.9684\nEpoch 35/100\n11/11 [==============================] - 0s 2ms/step - loss: 91.0983\nEpoch 36/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.9732\nEpoch 37/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.9472\nEpoch 38/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.9128\nEpoch 39/100\n11/11 [==============================] - 0s 2ms/step - loss: 91.0360\nEpoch 40/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.9132\nEpoch 41/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.9193\nEpoch 42/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.9364\nEpoch 43/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8977\nEpoch 44/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.9577\nEpoch 45/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.9994\nEpoch 46/100\n11/11 [==============================] - 0s 2ms/step - loss: 91.0100\nEpoch 47/100\n11/11 [==============================] - 0s 3ms/step - loss: 91.0025\nEpoch 48/100\n11/11 [==============================] - 0s 3ms/step - loss: 91.0074\nEpoch 49/100\n11/11 [==============================] - 0s 3ms/step - loss: 90.9457\nEpoch 50/100\n11/11 [==============================] - 0s 3ms/step - loss: 91.0306\nEpoch 51/100\n11/11 [==============================] - 0s 2ms/step - loss: 91.1645\nEpoch 52/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.9211\nEpoch 53/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.9330\nEpoch 54/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8901\nEpoch 55/100\n11/11 [==============================] - 0s 2ms/step - loss: 91.0190\nEpoch 56/100\n11/11 [==============================] - 0s 2ms/step - loss: 91.0044\nEpoch 57/100\n11/11 [==============================] - 0s 2ms/step - loss: 91.0272\nEpoch 58/100\n11/11 [==============================] - 0s 2ms/step - loss: 91.0088\nEpoch 59/100\n11/11 [==============================] - 0s 2ms/step - loss: 91.0335\nEpoch 60/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8989\nEpoch 61/100\n11/11 [==============================] - 0s 2ms/step - loss: 91.0687\nEpoch 62/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.9590\nEpoch 63/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.9457\nEpoch 64/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.9054\nEpoch 65/100\n11/11 [==============================] - 0s 2ms/step - loss: 91.0399\nEpoch 66/100\n11/11 [==============================] - 0s 1ms/step - loss: 90.9739\nEpoch 67/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.9935\nEpoch 68/100\n11/11 [==============================] - 0s 3ms/step - loss: 90.9428\nEpoch 69/100\n11/11 [==============================] - ETA: 0s - loss: 93.67 - 0s 2ms/step - loss: 90.9365\nEpoch 70/100\n11/11 [==============================] - 0s 3ms/step - loss: 90.9752\nEpoch 71/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.9980\nEpoch 72/100\n11/11 [==============================] - 0s 2ms/step - loss: 91.0044\nEpoch 73/100\n11/11 [==============================] - 0s 3ms/step - loss: 91.0282\nEpoch 74/100\n11/11 [==============================] - 0s 2ms/step - loss: 91.0151\nEpoch 75/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.9688\nEpoch 76/100\n11/11 [==============================] - 0s 1ms/step - loss: 90.9369\nEpoch 77/100\n11/11 [==============================] - 0s 2ms/step - loss: 91.0542\nEpoch 78/100\n11/11 [==============================] - 0s 3ms/step - loss: 90.9409\nEpoch 79/100\n11/11 [==============================] - 0s 2ms/step - loss: 91.1157\nEpoch 80/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.9322\nEpoch 81/100\n11/11 [==============================] - 0s 2ms/step - loss: 91.0418\nEpoch 82/100\n11/11 [==============================] - 0s 3ms/step - loss: 90.9078\nEpoch 83/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.9901\nEpoch 84/100\n11/11 [==============================] - 0s 3ms/step - loss: 91.0322\nEpoch 85/100\n11/11 [==============================] - 0s 2ms/step - loss: 91.0187\nEpoch 86/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.9214\nEpoch 87/100\n11/11 [==============================] - 0s 2ms/step - loss: 91.0568\nEpoch 88/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8874\nEpoch 89/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.9281\nEpoch 90/100\n11/11 [==============================] - 0s 2ms/step - loss: 91.0325\nEpoch 91/100\n11/11 [==============================] - 0s 3ms/step - loss: 90.9906\nEpoch 92/100\n11/11 [==============================] - 0s 2ms/step - loss: 91.0670\nEpoch 93/100\n11/11 [==============================] - 0s 3ms/step - loss: 90.8633\nEpoch 94/100\n11/11 [==============================] - 0s 3ms/step - loss: 90.9740\nEpoch 95/100\n11/11 [==============================] - 0s 8ms/step - loss: 91.0710\nEpoch 96/100\n11/11 [==============================] - 0s 3ms/step - loss: 90.9303\nEpoch 97/100\n11/11 [==============================] - 0s 3ms/step - loss: 90.9599\nEpoch 98/100\n11/11 [==============================] - 0s 3ms/step - loss: 90.9306\nEpoch 99/100\n11/11 [==============================] - 0s 2ms/step - loss: 91.0165\nEpoch 100/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.9525\nWARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fabcc186488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n3/3 [==============================] - 0s 2ms/step - loss: 71.1146\nThe MSE value is:  71.11458587646484\n","output_type":"stream"}]},{"cell_type":"markdown","source":"You can see above how the loss (cost) i.e. MSE has changed just by changing the epochs and keeping the learning rate same as 0.01 (i.e. the first model we built)","metadata":{"id":"CPakaxBaZZNA","colab_type":"text","cell_id":"00048-c6a2a3e9-87ca-4f3c-ad15-a7b21a7f9170"}},{"cell_type":"markdown","source":"### Exercise 2\n\nTest several epoch values to see the impact of varying this value when defining your model.","metadata":{"id":"6WoG362ohrQF","colab_type":"text","cell_id":"00049-6568d56b-a640-4fd0-9e24-91d2b8d91f02"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00055-60fb7c71-826a-4bbe-b345-4aa2f3410484"},"source":"epochs_list = [20, 50, 100, 200]","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QUmO4ehKqC2_","colab_type":"code","colab":{},"cell_id":"00050-d813e725-27b9-4063-bf07-9daeb1c27edf"},"source":"# Play with epochs\nfor i in epochs_list:\n\n    learning_rate = 0.01         \n    epochs =   i           # Replace ? with an integer\n    optimizer = RMSprop(learning_rate)\n    model.compile(loss='mean_squared_error',optimizer=optimizer)    # Compile the model\n    model.fit(X_train, y_train, epochs=epochs, batch_size=30)       # Fit the model\n    evalualte = model.evaluate(X_test, y_test)# Evaluate the model\n    print('The MSE value is: ',evalualte )","execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/20\n11/11 [==============================] - 0s 2ms/step - loss: 90.8849\nEpoch 2/20\n11/11 [==============================] - 0s 2ms/step - loss: 90.8493\nEpoch 3/20\n11/11 [==============================] - 0s 2ms/step - loss: 90.8444\nEpoch 4/20\n11/11 [==============================] - 0s 2ms/step - loss: 90.8543\nEpoch 5/20\n11/11 [==============================] - 0s 1ms/step - loss: 90.8388\nEpoch 6/20\n11/11 [==============================] - 0s 2ms/step - loss: 90.8450\nEpoch 7/20\n11/11 [==============================] - 0s 1ms/step - loss: 90.8508\nEpoch 8/20\n11/11 [==============================] - 0s 2ms/step - loss: 90.8480\nEpoch 9/20\n11/11 [==============================] - 0s 2ms/step - loss: 90.8469\nEpoch 10/20\n11/11 [==============================] - 0s 4ms/step - loss: 90.8425\nEpoch 11/20\n11/11 [==============================] - 0s 2ms/step - loss: 90.8429\nEpoch 12/20\n11/11 [==============================] - 0s 2ms/step - loss: 90.8419\nEpoch 13/20\n11/11 [==============================] - 0s 2ms/step - loss: 90.8536\nEpoch 14/20\n11/11 [==============================] - 0s 2ms/step - loss: 90.8621\nEpoch 15/20\n11/11 [==============================] - 0s 2ms/step - loss: 90.8396\nEpoch 16/20\n11/11 [==============================] - 0s 2ms/step - loss: 90.8431\nEpoch 17/20\n11/11 [==============================] - 0s 2ms/step - loss: 90.8445\nEpoch 18/20\n11/11 [==============================] - 0s 2ms/step - loss: 90.8375\nEpoch 19/20\n11/11 [==============================] - 0s 2ms/step - loss: 90.8440\nEpoch 20/20\n11/11 [==============================] - 0s 2ms/step - loss: 90.8401\nWARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fabcc2562f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n3/3 [==============================] - 0s 3ms/step - loss: 71.1060\nThe MSE value is:  71.10602569580078\nEpoch 1/50\n11/11 [==============================] - 0s 5ms/step - loss: 90.8865\nEpoch 2/50\n11/11 [==============================] - 0s 3ms/step - loss: 90.8509\nEpoch 3/50\n11/11 [==============================] - 0s 2ms/step - loss: 90.8458\nEpoch 4/50\n11/11 [==============================] - 0s 2ms/step - loss: 90.8555\nEpoch 5/50\n11/11 [==============================] - 0s 3ms/step - loss: 90.8400\nEpoch 6/50\n11/11 [==============================] - 0s 2ms/step - loss: 90.8459\nEpoch 7/50\n11/11 [==============================] - 0s 2ms/step - loss: 90.8516\nEpoch 8/50\n11/11 [==============================] - 0s 2ms/step - loss: 90.8487\nEpoch 9/50\n11/11 [==============================] - 0s 2ms/step - loss: 90.8475\nEpoch 10/50\n11/11 [==============================] - 0s 2ms/step - loss: 90.8430\nEpoch 11/50\n11/11 [==============================] - 0s 2ms/step - loss: 90.8434\nEpoch 12/50\n11/11 [==============================] - 0s 2ms/step - loss: 90.8424\nEpoch 13/50\n11/11 [==============================] - 0s 1ms/step - loss: 90.8541\nEpoch 14/50\n11/11 [==============================] - 0s 2ms/step - loss: 90.8625\nEpoch 15/50\n11/11 [==============================] - 0s 2ms/step - loss: 90.8399\nEpoch 16/50\n11/11 [==============================] - 0s 2ms/step - loss: 90.8433\nEpoch 17/50\n11/11 [==============================] - 0s 2ms/step - loss: 90.8447\nEpoch 18/50\n11/11 [==============================] - 0s 2ms/step - loss: 90.8377\nEpoch 19/50\n11/11 [==============================] - 0s 1ms/step - loss: 90.8442\nEpoch 20/50\n11/11 [==============================] - 0s 1ms/step - loss: 90.8403\nEpoch 21/50\n11/11 [==============================] - 0s 2ms/step - loss: 90.8513\nEpoch 22/50\n11/11 [==============================] - 0s 2ms/step - loss: 90.8408\nEpoch 23/50\n11/11 [==============================] - 0s 2ms/step - loss: 90.8442\nEpoch 24/50\n11/11 [==============================] - 0s 2ms/step - loss: 90.8403\nEpoch 25/50\n11/11 [==============================] - 0s 2ms/step - loss: 90.8432\nEpoch 26/50\n11/11 [==============================] - 0s 2ms/step - loss: 90.8377\nEpoch 27/50\n11/11 [==============================] - 0s 2ms/step - loss: 90.8381\nEpoch 28/50\n11/11 [==============================] - 0s 2ms/step - loss: 90.8629\nEpoch 29/50\n11/11 [==============================] - 0s 1ms/step - loss: 90.8503\nEpoch 30/50\n11/11 [==============================] - 0s 2ms/step - loss: 90.8520\nEpoch 31/50\n11/11 [==============================] - 0s 1ms/step - loss: 90.8393\nEpoch 32/50\n11/11 [==============================] - 0s 2ms/step - loss: 90.8474\nEpoch 33/50\n11/11 [==============================] - 0s 2ms/step - loss: 90.8439\nEpoch 34/50\n11/11 [==============================] - 0s 2ms/step - loss: 90.8434\nEpoch 35/50\n11/11 [==============================] - 0s 1ms/step - loss: 90.8566\nEpoch 36/50\n11/11 [==============================] - 0s 1ms/step - loss: 90.8446\nEpoch 37/50\n11/11 [==============================] - 0s 1ms/step - loss: 90.8411\nEpoch 38/50\n11/11 [==============================] - 0s 1ms/step - loss: 90.8381\nEpoch 39/50\n11/11 [==============================] - 0s 1ms/step - loss: 90.8507\nEpoch 40/50\n11/11 [==============================] - 0s 1ms/step - loss: 90.8388\nEpoch 41/50\n11/11 [==============================] - 0s 2ms/step - loss: 90.8399\nEpoch 42/50\n11/11 [==============================] - 0s 1ms/step - loss: 90.8418\nEpoch 43/50\n11/11 [==============================] - 0s 1ms/step - loss: 90.8363\nEpoch 44/50\n11/11 [==============================] - 0s 2ms/step - loss: 90.8424\nEpoch 45/50\n11/11 [==============================] - 0s 2ms/step - loss: 90.8465\nEpoch 46/50\n11/11 [==============================] - 0s 2ms/step - loss: 90.8488\nEpoch 47/50\n11/11 [==============================] - 0s 2ms/step - loss: 90.8473\nEpoch 48/50\n11/11 [==============================] - 0s 2ms/step - loss: 90.8499\nEpoch 49/50\n11/11 [==============================] - 0s 3ms/step - loss: 90.8410\nEpoch 50/50\n11/11 [==============================] - 0s 2ms/step - loss: 90.8498\nWARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fabcd5e7a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n3/3 [==============================] - 0s 4ms/step - loss: 71.1067\nThe MSE value is:  71.10670471191406\nEpoch 1/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8872\nEpoch 2/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8515\nEpoch 3/100\n11/11 [==============================] - 0s 1ms/step - loss: 90.8463\nEpoch 4/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8560\nEpoch 5/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8405\nEpoch 6/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8463\nEpoch 7/100\n11/11 [==============================] - 0s 1ms/step - loss: 90.8520\nEpoch 8/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8490\nEpoch 9/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8477\nEpoch 10/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8433\nEpoch 11/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8436\nEpoch 12/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8426\nEpoch 13/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8543\nEpoch 14/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8627\nEpoch 15/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8400\nEpoch 16/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8434\nEpoch 17/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8448\nEpoch 18/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8378\nEpoch 19/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8443\nEpoch 20/100\n11/11 [==============================] - 0s 3ms/step - loss: 90.8403\nEpoch 21/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8513\nEpoch 22/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8409\nEpoch 23/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8442\nEpoch 24/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8404\nEpoch 25/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8432\nEpoch 26/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8378\nEpoch 27/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8382\nEpoch 28/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8629\nEpoch 29/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8503\nEpoch 30/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8520\nEpoch 31/100\n11/11 [==============================] - 0s 3ms/step - loss: 90.8393\nEpoch 32/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8474\nEpoch 33/100\n11/11 [==============================] - 0s 3ms/step - loss: 90.8439\nEpoch 34/100\n11/11 [==============================] - 0s 3ms/step - loss: 90.8434\nEpoch 35/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8567\nEpoch 36/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8446\nEpoch 37/100\n11/11 [==============================] - 0s 3ms/step - loss: 90.8411\nEpoch 38/100\n11/11 [==============================] - 0s 3ms/step - loss: 90.8381\nEpoch 39/100\n11/11 [==============================] - 0s 3ms/step - loss: 90.8507\nEpoch 40/100\n11/11 [==============================] - 0s 3ms/step - loss: 90.8388\nEpoch 41/100\n11/11 [==============================] - 0s 3ms/step - loss: 90.8400\nEpoch 42/100\n11/11 [==============================] - 0s 3ms/step - loss: 90.8418\nEpoch 43/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8363\nEpoch 44/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8424\nEpoch 45/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8465\nEpoch 46/100\n11/11 [==============================] - 0s 3ms/step - loss: 90.8488\nEpoch 47/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8474\nEpoch 48/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8499\nEpoch 49/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8410\nEpoch 50/100\n11/11 [==============================] - 0s 3ms/step - loss: 90.8498\nEpoch 51/100\n11/11 [==============================] - 0s 3ms/step - loss: 90.8638\nEpoch 52/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8384\nEpoch 53/100\n11/11 [==============================] - 0s 3ms/step - loss: 90.8401\nEpoch 54/100\n11/11 [==============================] - 0s 4ms/step - loss: 90.8355\nEpoch 55/100\n11/11 [==============================] - 0s 3ms/step - loss: 90.8490\nEpoch 56/100\n11/11 [==============================] - 0s 3ms/step - loss: 90.8477\nEpoch 57/100\n11/11 [==============================] - 0s 3ms/step - loss: 90.8487\nEpoch 58/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8476\nEpoch 59/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8493\nEpoch 60/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8362\nEpoch 61/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8540\nEpoch 62/100\n11/11 [==============================] - 0s 1ms/step - loss: 90.8425\nEpoch 63/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8402\nEpoch 64/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8360\nEpoch 65/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8527\nEpoch 66/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8429\nEpoch 67/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8457\nEpoch 68/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8425\nEpoch 69/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8403\nEpoch 70/100\n11/11 [==============================] - 0s 1ms/step - loss: 90.8460\nEpoch 71/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8457\nEpoch 72/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8460\nEpoch 73/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8486\nEpoch 74/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8472\nEpoch 75/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8435\nEpoch 76/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8402\nEpoch 77/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8511\nEpoch 78/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8391\nEpoch 79/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8578\nEpoch 80/100\n11/11 [==============================] - 0s 3ms/step - loss: 90.8403\nEpoch 81/100\n11/11 [==============================] - 0s 3ms/step - loss: 90.8504\nEpoch 82/100\n11/11 [==============================] - 0s 3ms/step - loss: 90.8371\nEpoch 83/100\n11/11 [==============================] - 0s 6ms/step - loss: 90.8459\nEpoch 84/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8494\nEpoch 85/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8490\nEpoch 86/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8382\nEpoch 87/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8556\nEpoch 88/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8349\nEpoch 89/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8388\nEpoch 90/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8503\nEpoch 91/100\n11/11 [==============================] - 0s 6ms/step - loss: 90.8451\nEpoch 92/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8567\nEpoch 93/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8324\nEpoch 94/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8435\nEpoch 95/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8540\nEpoch 96/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8418\nEpoch 97/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8413\nEpoch 98/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8410\nEpoch 99/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8476\nEpoch 100/100\n11/11 [==============================] - 0s 2ms/step - loss: 90.8415\nWARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fabcc538510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n3/3 [==============================] - 0s 3ms/step - loss: 71.1049\nThe MSE value is:  71.1048812866211\nEpoch 1/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8854\nEpoch 2/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8498\nEpoch 3/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8448\nEpoch 4/200\n11/11 [==============================] - 0s 1ms/step - loss: 90.8546\nEpoch 5/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8392\nEpoch 6/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8453\nEpoch 7/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8511\nEpoch 8/200\n11/11 [==============================] - 0s 1ms/step - loss: 90.8482\nEpoch 9/200\n11/11 [==============================] - 0s 1ms/step - loss: 90.8471\nEpoch 10/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8427\nEpoch 11/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8431\nEpoch 12/200\n11/11 [==============================] - 0s 1ms/step - loss: 90.8421\nEpoch 13/200\n11/11 [==============================] - 0s 1ms/step - loss: 90.8538\nEpoch 14/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8622\nEpoch 15/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8397\nEpoch 16/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8431\nEpoch 17/200\n11/11 [==============================] - 0s 1ms/step - loss: 90.8446\nEpoch 18/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8376\nEpoch 19/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8440\nEpoch 20/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8401\nEpoch 21/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8512\nEpoch 22/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8407\nEpoch 23/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8440\nEpoch 24/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8402\nEpoch 25/200\n11/11 [==============================] - 0s 5ms/step - loss: 90.8431\nEpoch 26/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8377\nEpoch 27/200\n11/11 [==============================] - 0s 4ms/step - loss: 90.8381\nEpoch 28/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8628\nEpoch 29/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8502\nEpoch 30/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8519\nEpoch 31/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8392\nEpoch 32/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8473\nEpoch 33/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8438\nEpoch 34/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8433\nEpoch 35/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8566\nEpoch 36/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8445\nEpoch 37/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8410\nEpoch 38/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8380\nEpoch 39/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8506\nEpoch 40/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8387\nEpoch 41/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8399\nEpoch 42/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8418\nEpoch 43/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8363\nEpoch 44/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8424\nEpoch 45/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8464\nEpoch 46/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8487\nEpoch 47/200\n11/11 [==============================] - 0s 4ms/step - loss: 90.8473\nEpoch 48/200\n11/11 [==============================] - 0s 4ms/step - loss: 90.8499\nEpoch 49/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8409\nEpoch 50/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8497\nEpoch 51/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8638\nEpoch 52/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8384\nEpoch 53/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8401\nEpoch 54/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8355\nEpoch 55/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8490\nEpoch 56/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8477\nEpoch 57/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8487\nEpoch 58/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8476\nEpoch 59/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8493\nEpoch 60/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8362\nEpoch 61/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8540\nEpoch 62/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8425\nEpoch 63/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8402\nEpoch 64/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8360\nEpoch 65/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8527\nEpoch 66/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8429\nEpoch 67/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8457\nEpoch 68/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8425\nEpoch 69/200\n11/11 [==============================] - ETA: 0s - loss: 93.92 - 0s 2ms/step - loss: 90.8403\nEpoch 70/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8460\nEpoch 71/200\n11/11 [==============================] - ETA: 0s - loss: 101.404 - 0s 2ms/step - loss: 90.8457\nEpoch 72/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8460\nEpoch 73/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8486\nEpoch 74/200\n11/11 [==============================] - 0s 5ms/step - loss: 90.8472\nEpoch 75/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8435\nEpoch 76/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8402\nEpoch 77/200\n11/11 [==============================] - 0s 4ms/step - loss: 90.8511\nEpoch 78/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8390\nEpoch 79/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8578\nEpoch 80/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8403\nEpoch 81/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8504\nEpoch 82/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8371\nEpoch 83/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8459\nEpoch 84/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8494\nEpoch 85/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8490\nEpoch 86/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8382\nEpoch 87/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8556\nEpoch 88/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8349\nEpoch 89/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8388\nEpoch 90/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8503\nEpoch 91/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8451\nEpoch 92/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8567\nEpoch 93/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8324\nEpoch 94/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8435\nEpoch 95/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8540\nEpoch 96/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8418\nEpoch 97/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8413\nEpoch 98/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8410\nEpoch 99/200\n11/11 [==============================] - 0s 4ms/step - loss: 90.8476\nEpoch 100/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8415\nEpoch 101/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8559\nEpoch 102/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8413\nEpoch 103/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8439\nEpoch 104/200\n11/11 [==============================] - 0s 1ms/step - loss: 90.8447\nEpoch 105/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8442\nEpoch 106/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8474\nEpoch 107/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8459\nEpoch 108/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8402\nEpoch 109/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8538\nEpoch 110/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8466\nEpoch 111/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8538\nEpoch 112/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8428\nEpoch 113/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8450\nEpoch 114/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8390\nEpoch 115/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8503\nEpoch 116/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8381\nEpoch 117/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8440\nEpoch 118/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8471\nEpoch 119/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8456\nEpoch 120/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8501\nEpoch 121/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8370\nEpoch 122/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8449\nEpoch 123/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8407\nEpoch 124/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8431\nEpoch 125/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8437\nEpoch 126/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8447\nEpoch 127/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8532\nEpoch 128/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8429\nEpoch 129/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8532\nEpoch 130/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8397\nEpoch 131/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8448\nEpoch 132/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8414\nEpoch 133/200\n11/11 [==============================] - 0s 1ms/step - loss: 90.8389\nEpoch 134/200\n11/11 [==============================] - 0s 1ms/step - loss: 90.8533\nEpoch 135/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8540\nEpoch 136/200\n11/11 [==============================] - 0s 1ms/step - loss: 90.8397\nEpoch 137/200\n11/11 [==============================] - 0s 1ms/step - loss: 90.8437\nEpoch 138/200\n11/11 [==============================] - 0s 1ms/step - loss: 90.8511\nEpoch 139/200\n11/11 [==============================] - 0s 1ms/step - loss: 90.8439\nEpoch 140/200\n11/11 [==============================] - 0s 1ms/step - loss: 90.8354\nEpoch 141/200\n11/11 [==============================] - 0s 1ms/step - loss: 90.8458\nEpoch 142/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8483\nEpoch 143/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8408\nEpoch 144/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8386\nEpoch 145/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8401\nEpoch 146/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8520\nEpoch 147/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8417\nEpoch 148/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8413\nEpoch 149/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8414\nEpoch 150/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8575\nEpoch 151/200\n11/11 [==============================] - 0s 4ms/step - loss: 90.8536\nEpoch 152/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8329\nEpoch 153/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8377\nEpoch 154/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8480\nEpoch 155/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8563\nEpoch 156/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8424\nEpoch 157/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8506\nEpoch 158/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8472\nEpoch 159/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8400\nEpoch 160/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8362\nEpoch 161/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8428\nEpoch 162/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8414\nEpoch 163/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8388\nEpoch 164/200\n11/11 [==============================] - 0s 4ms/step - loss: 90.8476\nEpoch 165/200\n11/11 [==============================] - 0s 4ms/step - loss: 90.8483\nEpoch 166/200\n11/11 [==============================] - 0s 4ms/step - loss: 90.8440\nEpoch 167/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8356\nEpoch 168/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8586\nEpoch 169/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8413\nEpoch 170/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8575\nEpoch 171/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8475\nEpoch 172/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8367\nEpoch 173/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8509\nEpoch 174/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8504\nEpoch 175/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8357\nEpoch 176/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8393\nEpoch 177/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8477\nEpoch 178/200\n11/11 [==============================] - 0s 4ms/step - loss: 90.8400\nEpoch 179/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8454\nEpoch 180/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8537\nEpoch 181/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8418\nEpoch 182/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8400\nEpoch 183/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8361\nEpoch 184/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8490\nEpoch 185/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8472\nEpoch 186/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8502\nEpoch 187/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8421\nEpoch 188/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8425\nEpoch 189/200\n11/11 [==============================] - 0s 3ms/step - loss: 90.8444\nEpoch 190/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8382\nEpoch 191/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8381\nEpoch 192/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8377\nEpoch 193/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8594\nEpoch 194/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8405\nEpoch 195/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8427\nEpoch 196/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8501\nEpoch 197/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8454\nEpoch 198/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8446\nEpoch 199/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8317\nEpoch 200/200\n11/11 [==============================] - 0s 2ms/step - loss: 90.8458\nWARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fabcc2c1598> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n3/3 [==============================] - 0s 2ms/step - loss: 71.1044\nThe MSE value is:  71.10435485839844\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00057-846a4784-75b6-4fb6-ae29-2b44236c1306"},"source":"epochs_list1 = [500, 800, 1000]","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00058-a3a57ed3-3d62-4fd3-bd8f-73ee08b3a9f2"},"source":"# Play with epochs\nfor i in epochs_list1:\n\n    learning_rate = 0.01         \n    epochs =   i           # Replace ? with an integer\n    optimizer = RMSprop(learning_rate)\n    model.compile(loss='mean_squared_error',optimizer=optimizer)    # Compile the model\n    model.fit(X_train, y_train, epochs=epochs, batch_size=30)       # Fit the model\n    evalualte = model.evaluate(X_test, y_test)# Evaluate the model\n    print('The MSE value is: ',evalualte )","execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 503/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8357\nEpoch 504/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8520\nEpoch 505/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8447\nEpoch 506/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8512\nEpoch 507/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8550\nEpoch 508/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8369\nEpoch 509/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8444\nEpoch 510/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8379\nEpoch 511/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8410\nEpoch 512/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8373\nEpoch 513/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8500\nEpoch 514/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8381\nEpoch 515/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8408\nEpoch 516/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8426\nEpoch 517/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8442\nEpoch 518/1000\n11/11 [==============================] - 0s 1ms/step - loss: 90.8369\nEpoch 519/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8573\nEpoch 520/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8400\nEpoch 521/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8505\nEpoch 522/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8501\nEpoch 523/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8391\nEpoch 524/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8464\nEpoch 525/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8407\nEpoch 526/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8465\nEpoch 527/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8479\nEpoch 528/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8348\nEpoch 529/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8370\nEpoch 530/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8430\nEpoch 531/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8393\nEpoch 532/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8536\nEpoch 533/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8393\nEpoch 534/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8523\nEpoch 535/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8359\nEpoch 536/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8482\nEpoch 537/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8389\nEpoch 538/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8448\nEpoch 539/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8401\nEpoch 540/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8454\nEpoch 541/1000\n11/11 [==============================] - 0s 9ms/step - loss: 90.8521\nEpoch 542/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8474\nEpoch 543/1000\n11/11 [==============================] - 0s 5ms/step - loss: 90.8429\nEpoch 544/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8405\nEpoch 545/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8469\nEpoch 546/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8453\nEpoch 547/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8489\nEpoch 548/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8379\nEpoch 549/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8457\nEpoch 550/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8465\nEpoch 551/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8374\nEpoch 552/1000\n11/11 [==============================] - 0s 8ms/step - loss: 90.8423\nEpoch 553/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8407\nEpoch 554/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8499\nEpoch 555/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8504\nEpoch 556/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8384\nEpoch 557/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8529\nEpoch 558/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8444\nEpoch 559/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8426\nEpoch 560/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8462\nEpoch 561/1000\n11/11 [==============================] - 0s 5ms/step - loss: 90.8471\nEpoch 562/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8452\nEpoch 563/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8408\nEpoch 564/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8403\nEpoch 565/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8376\nEpoch 566/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8431\nEpoch 567/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8418\nEpoch 568/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8610\nEpoch 569/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8441\nEpoch 570/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8401\nEpoch 571/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8451\nEpoch 572/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8374\nEpoch 573/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8417\nEpoch 574/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8445\nEpoch 575/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8530\nEpoch 576/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8396\nEpoch 577/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8359\nEpoch 578/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8484\nEpoch 579/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8471\nEpoch 580/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8430\nEpoch 581/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8452\nEpoch 582/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8427\nEpoch 583/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8367\nEpoch 584/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8527\nEpoch 585/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8461\nEpoch 586/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8354\nEpoch 587/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8449\nEpoch 588/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8475\nEpoch 589/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8532\nEpoch 590/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8363\nEpoch 591/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8410\nEpoch 592/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8404\nEpoch 593/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8409\nEpoch 594/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8405\nEpoch 595/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8489\nEpoch 596/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8525\nEpoch 597/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8424\nEpoch 598/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8334\nEpoch 599/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8323\nEpoch 600/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8726\nEpoch 601/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8356\nEpoch 602/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8420\nEpoch 603/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8518\nEpoch 604/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8388\nEpoch 605/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8427\nEpoch 606/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8412\nEpoch 607/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8510\nEpoch 608/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8420\nEpoch 609/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8448\nEpoch 610/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8453\nEpoch 611/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8392\nEpoch 612/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8352\nEpoch 613/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8466\nEpoch 614/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8543\nEpoch 615/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8466\nEpoch 616/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8486\nEpoch 617/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8379\nEpoch 618/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8521\nEpoch 619/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8537\nEpoch 620/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8504\nEpoch 621/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8495\nEpoch 622/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8429\nEpoch 623/1000\n11/11 [==============================] - 0s 5ms/step - loss: 90.8515\nEpoch 624/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8448\nEpoch 625/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8407\nEpoch 626/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8422\nEpoch 627/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8525\nEpoch 628/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8480\nEpoch 629/1000\n11/11 [==============================] - 0s 7ms/step - loss: 90.8395\nEpoch 630/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8528\nEpoch 631/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8441\nEpoch 632/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8430\nEpoch 633/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8501\nEpoch 634/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8435\nEpoch 635/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8550\nEpoch 636/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8482\nEpoch 637/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8366\nEpoch 638/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8447\nEpoch 639/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8359\nEpoch 640/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8449\nEpoch 641/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8483\nEpoch 642/1000\n11/11 [==============================] - 0s 9ms/step - loss: 90.8441\nEpoch 643/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8427\nEpoch 644/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8462\nEpoch 645/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8443\nEpoch 646/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8508\nEpoch 647/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8384\nEpoch 648/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8520\nEpoch 649/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8454\nEpoch 650/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8412\nEpoch 651/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8553\nEpoch 652/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8346\nEpoch 653/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8410\nEpoch 654/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8449\nEpoch 655/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8411\nEpoch 656/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8457\nEpoch 657/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8408\nEpoch 658/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8543\nEpoch 659/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8404\nEpoch 660/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8433\nEpoch 661/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8415\nEpoch 662/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8449\nEpoch 663/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8394\nEpoch 664/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8414\nEpoch 665/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8363\nEpoch 666/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8554\nEpoch 667/1000\n11/11 [==============================] - ETA: 0s - loss: 91.83 - 0s 4ms/step - loss: 90.8402\nEpoch 668/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8410\nEpoch 669/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8462\nEpoch 670/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8458\nEpoch 671/1000\n11/11 [==============================] - 0s 6ms/step - loss: 90.8466\nEpoch 672/1000\n11/11 [==============================] - 0s 5ms/step - loss: 90.8430\nEpoch 673/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8333\nEpoch 674/1000\n11/11 [==============================] - 0s 5ms/step - loss: 90.8466\nEpoch 675/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8401\nEpoch 676/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8502\nEpoch 677/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8471\nEpoch 678/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8408\nEpoch 679/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8550\nEpoch 680/1000\n11/11 [==============================] - 0s 7ms/step - loss: 90.8439\nEpoch 681/1000\n11/11 [==============================] - 0s 13ms/step - loss: 90.8471\nEpoch 682/1000\n11/11 [==============================] - 0s 6ms/step - loss: 90.8433\nEpoch 683/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8404\nEpoch 684/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8467\nEpoch 685/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8444\nEpoch 686/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8385\nEpoch 687/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8393\nEpoch 688/1000\n11/11 [==============================] - 0s 7ms/step - loss: 90.8422\nEpoch 689/1000\n11/11 [==============================] - 0s 6ms/step - loss: 90.8407\nEpoch 690/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8427\nEpoch 691/1000\n11/11 [==============================] - 0s 5ms/step - loss: 90.8448\nEpoch 692/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8523\nEpoch 693/1000\n11/11 [==============================] - ETA: 0s - loss: 124.583 - 0s 3ms/step - loss: 90.8431\nEpoch 694/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8355\nEpoch 695/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8420\nEpoch 696/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8577\nEpoch 697/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8372\nEpoch 698/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8381\nEpoch 699/1000\n11/11 [==============================] - 0s 8ms/step - loss: 90.8438\nEpoch 700/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8471\nEpoch 701/1000\n11/11 [==============================] - 0s 6ms/step - loss: 90.8405\nEpoch 702/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8528\nEpoch 703/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8380\nEpoch 704/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8467\nEpoch 705/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8408\nEpoch 706/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8438\nEpoch 707/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8438\nEpoch 708/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8484\nEpoch 709/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8457\nEpoch 710/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8391\nEpoch 711/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8417\nEpoch 712/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8531\nEpoch 713/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8409\nEpoch 714/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8404\nEpoch 715/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8339\nEpoch 716/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8480\nEpoch 717/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8418\nEpoch 718/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8387\nEpoch 719/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8470\nEpoch 720/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8484\nEpoch 721/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8409\nEpoch 722/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8486\nEpoch 723/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8426\nEpoch 724/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8381\nEpoch 725/1000\n11/11 [==============================] - ETA: 0s - loss: 93.02 - 0s 2ms/step - loss: 90.8456\nEpoch 726/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8482\nEpoch 727/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8445\nEpoch 728/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8405\nEpoch 729/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8561\nEpoch 730/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8385\nEpoch 731/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8440\nEpoch 732/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8486\nEpoch 733/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8308\nEpoch 734/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8429\nEpoch 735/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8463\nEpoch 736/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8379\nEpoch 737/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8571\nEpoch 738/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8429\nEpoch 739/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8492\nEpoch 740/1000\n11/11 [==============================] - 0s 1ms/step - loss: 90.8364\nEpoch 741/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8558\nEpoch 742/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8434\nEpoch 743/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8359\nEpoch 744/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8388\nEpoch 745/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8415\nEpoch 746/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8445\nEpoch 747/1000\n11/11 [==============================] - ETA: 0s - loss: 79.28 - 0s 2ms/step - loss: 90.8454\nEpoch 748/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8429\nEpoch 749/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8387\nEpoch 750/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8490\nEpoch 751/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8501\nEpoch 752/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8587\nEpoch 753/1000\n11/11 [==============================] - 0s 5ms/step - loss: 90.8405\nEpoch 754/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8452\nEpoch 755/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8408\nEpoch 756/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8457\nEpoch 757/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8401\nEpoch 758/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8455\nEpoch 759/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8339\nEpoch 760/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8426\nEpoch 761/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8509\nEpoch 762/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8421\nEpoch 763/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8403\nEpoch 764/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8355\nEpoch 765/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8491\nEpoch 766/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8494\nEpoch 767/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8413\nEpoch 768/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8478\nEpoch 769/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8384\nEpoch 770/1000\n11/11 [==============================] - 0s 6ms/step - loss: 90.8501\nEpoch 771/1000\n11/11 [==============================] - 0s 10ms/step - loss: 90.8459\nEpoch 772/1000\n11/11 [==============================] - 0s 7ms/step - loss: 90.8455\nEpoch 773/1000\n11/11 [==============================] - 0s 6ms/step - loss: 90.8386\nEpoch 774/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8563\nEpoch 775/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8516\nEpoch 776/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8401\nEpoch 777/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8401\nEpoch 778/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8450\nEpoch 779/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8530\nEpoch 780/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8413\nEpoch 781/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8397\nEpoch 782/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8490\nEpoch 783/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8358\nEpoch 784/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8379\nEpoch 785/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8380\nEpoch 786/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8490\nEpoch 787/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8556\nEpoch 788/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8419\nEpoch 789/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8387\nEpoch 790/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8353\nEpoch 791/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8538\nEpoch 792/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8416\nEpoch 793/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8460\nEpoch 794/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8371\nEpoch 795/1000\n11/11 [==============================] - ETA: 0s - loss: 70.48 - 0s 3ms/step - loss: 90.8424\nEpoch 796/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8384\nEpoch 797/1000\n11/11 [==============================] - ETA: 0s - loss: 92.89 - 0s 2ms/step - loss: 90.8455\nEpoch 798/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8473\nEpoch 799/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8437\nEpoch 800/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8483\nEpoch 801/1000\n11/11 [==============================] - 0s 6ms/step - loss: 90.8369\nEpoch 802/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8524\nEpoch 803/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8398\nEpoch 804/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8397\nEpoch 805/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8523\nEpoch 806/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8430\nEpoch 807/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8407\nEpoch 808/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8431\nEpoch 809/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8510\nEpoch 810/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8403\nEpoch 811/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8440\nEpoch 812/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8516\nEpoch 813/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8453\nEpoch 814/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8408\nEpoch 815/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8437\nEpoch 816/1000\n11/11 [==============================] - 0s 5ms/step - loss: 90.8492\nEpoch 817/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8365\nEpoch 818/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8381\nEpoch 819/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8441\nEpoch 820/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8486\nEpoch 821/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8462\nEpoch 822/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8386\nEpoch 823/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8412\nEpoch 824/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8492\nEpoch 825/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8382\nEpoch 826/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8370\nEpoch 827/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8423\nEpoch 828/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8553\nEpoch 829/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8457\nEpoch 830/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8400\nEpoch 831/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8449\nEpoch 832/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8415\nEpoch 833/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8446\nEpoch 834/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8557\nEpoch 835/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8420\nEpoch 836/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8419\nEpoch 837/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8478\nEpoch 838/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8402\nEpoch 839/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8480\nEpoch 840/1000\n11/11 [==============================] - 0s 6ms/step - loss: 90.8416\nEpoch 841/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8354\nEpoch 842/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8387\nEpoch 843/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8493\nEpoch 844/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8534\nEpoch 845/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8391\nEpoch 846/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8353\nEpoch 847/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8373\nEpoch 848/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8390\nEpoch 849/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8505\nEpoch 850/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8453\nEpoch 851/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8431\nEpoch 852/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8499\nEpoch 853/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8435\nEpoch 854/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8342\nEpoch 855/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8476\nEpoch 856/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8447\nEpoch 857/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8515\nEpoch 858/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8544\nEpoch 859/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8342\nEpoch 860/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8424\nEpoch 861/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8440\nEpoch 862/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8406\nEpoch 863/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8483\nEpoch 864/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8525\nEpoch 865/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8415\nEpoch 866/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8348\nEpoch 867/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8411\nEpoch 868/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8467\nEpoch 869/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8487\nEpoch 870/1000\n11/11 [==============================] - 0s 7ms/step - loss: 90.8476\nEpoch 871/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8425\nEpoch 872/1000\n11/11 [==============================] - ETA: 0s - loss: 78.22 - 0s 2ms/step - loss: 90.8368\nEpoch 873/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8541\nEpoch 874/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8427\nEpoch 875/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8404\nEpoch 876/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8521\nEpoch 877/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8421\nEpoch 878/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8458\nEpoch 879/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8485\nEpoch 880/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8432\nEpoch 881/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8515\nEpoch 882/1000\n11/11 [==============================] - 0s 5ms/step - loss: 90.8426\nEpoch 883/1000\n11/11 [==============================] - 0s 5ms/step - loss: 90.8437\nEpoch 884/1000\n11/11 [==============================] - 0s 5ms/step - loss: 90.8403\nEpoch 885/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8375\nEpoch 886/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8439\nEpoch 887/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8437\nEpoch 888/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8426\nEpoch 889/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8456\nEpoch 890/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8458\nEpoch 891/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8474\nEpoch 892/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8357\nEpoch 893/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8411\nEpoch 894/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8492\nEpoch 895/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8344\nEpoch 896/1000\n11/11 [==============================] - 0s 7ms/step - loss: 90.8429\nEpoch 897/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8599\nEpoch 898/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8402\nEpoch 899/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8431\nEpoch 900/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8452\nEpoch 901/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8377\nEpoch 902/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8474\nEpoch 903/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8421\nEpoch 904/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8503\nEpoch 905/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8464\nEpoch 906/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8494\nEpoch 907/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8359\nEpoch 908/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8404\nEpoch 909/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8465\nEpoch 910/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8346\nEpoch 911/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8645\nEpoch 912/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8412\nEpoch 913/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8455\nEpoch 914/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8430\nEpoch 915/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8472\nEpoch 916/1000\n11/11 [==============================] - 0s 7ms/step - loss: 90.8434\nEpoch 917/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8472\nEpoch 918/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8421\nEpoch 919/1000\n11/11 [==============================] - 0s 6ms/step - loss: 90.8434\nEpoch 920/1000\n11/11 [==============================] - 0s 6ms/step - loss: 90.8491\nEpoch 921/1000\n11/11 [==============================] - 0s 6ms/step - loss: 90.8410\nEpoch 922/1000\n11/11 [==============================] - ETA: 0s - loss: 69.97 - 0s 3ms/step - loss: 90.8438\nEpoch 923/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8516\nEpoch 924/1000\n11/11 [==============================] - ETA: 0s - loss: 97.03 - 0s 2ms/step - loss: 90.8508\nEpoch 925/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8392\nEpoch 926/1000\n11/11 [==============================] - 0s 6ms/step - loss: 90.8409\nEpoch 927/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8385\nEpoch 928/1000\n11/11 [==============================] - ETA: 0s - loss: 109.004 - 0s 3ms/step - loss: 90.8428\nEpoch 929/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8535\nEpoch 930/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8478\nEpoch 931/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8390\nEpoch 932/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8474\nEpoch 933/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8497\nEpoch 934/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8352\nEpoch 935/1000\n11/11 [==============================] - 0s 5ms/step - loss: 90.8531\nEpoch 936/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8466\nEpoch 937/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8530\nEpoch 938/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8369\nEpoch 939/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8451\nEpoch 940/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8447\nEpoch 941/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8351\nEpoch 942/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8366\nEpoch 943/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8479\nEpoch 944/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8446\nEpoch 945/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8349\nEpoch 946/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8519\nEpoch 947/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8388\nEpoch 948/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8340\nEpoch 949/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8552\nEpoch 950/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8463\nEpoch 951/1000\n11/11 [==============================] - 0s 5ms/step - loss: 90.8405\nEpoch 952/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8433\nEpoch 953/1000\n11/11 [==============================] - 0s 5ms/step - loss: 90.8440\nEpoch 954/1000\n11/11 [==============================] - 0s 6ms/step - loss: 90.8466\nEpoch 955/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8460\nEpoch 956/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8406\nEpoch 957/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8423\nEpoch 958/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8399\nEpoch 959/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8659\nEpoch 960/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8397\nEpoch 961/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8353\nEpoch 962/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8457\nEpoch 963/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8432\nEpoch 964/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8398\nEpoch 965/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8606\nEpoch 966/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8342\nEpoch 967/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8502\nEpoch 968/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8454\nEpoch 969/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8381\nEpoch 970/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8508\nEpoch 971/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8373\nEpoch 972/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8442\nEpoch 973/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8505\nEpoch 974/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8436\nEpoch 975/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8410\nEpoch 976/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8414\nEpoch 977/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8424\nEpoch 978/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8387\nEpoch 979/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8427\nEpoch 980/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8445\nEpoch 981/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8398\nEpoch 982/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8399\nEpoch 983/1000\n11/11 [==============================] - 0s 2ms/step - loss: 90.8398\nEpoch 984/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8580\nEpoch 985/1000\n11/11 [==============================] - 0s 7ms/step - loss: 90.8484: 0s - loss: 85.41\nEpoch 986/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8430\nEpoch 987/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8418\nEpoch 988/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8431\nEpoch 989/1000\n11/11 [==============================] - 0s 8ms/step - loss: 90.8445TA: 0s - loss: 101.668\nEpoch 990/1000\n11/11 [==============================] - 0s 5ms/step - loss: 90.8496\nEpoch 991/1000\n11/11 [==============================] - 0s 8ms/step - loss: 90.8511\nEpoch 992/1000\n11/11 [==============================] - 0s 7ms/step - loss: 90.8378\nEpoch 993/1000\n11/11 [==============================] - 0s 18ms/step - loss: 90.8343\nEpoch 994/1000\n11/11 [==============================] - 0s 10ms/step - loss: 90.8488\nEpoch 995/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8389\nEpoch 996/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8484\nEpoch 997/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8477\nEpoch 998/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8397\nEpoch 999/1000\n11/11 [==============================] - 0s 4ms/step - loss: 90.8472\nEpoch 1000/1000\n11/11 [==============================] - 0s 3ms/step - loss: 90.8384\nWARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fab952ff6a8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n3/3 [==============================] - 0s 5ms/step - loss: 71.1059\nThe MSE value is:  71.10594177246094\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Exercise 3\n\nFind the best possible combination of *learning rate* and *epochs* while testing some combinations","metadata":{"id":"K6cx5vbtqdeD","colab_type":"text","cell_id":"00051-0114fa65-7471-42fd-ba9a-fedf632e4b0e"}},{"cell_type":"code","source":"def combination(learning_rate , epochs):\n    # play with learning rate and epochs\n    #learning_rate = a        # Replace ? with a floating-point number\n    #epochs = b             # Replace ? with an integer\n    optimizer = RMSprop(learning_rate)\n    model.compile(loss='mean_squared_error',optimizer=optimizer)    # Compile the model\n    model.fit(X_train, y_train, epochs=epochs, batch_size=30)       # Fit the model\n    model.evaluate(X_test, y_test)                                  # Evaluate the model","metadata":{"tags":[],"cell_id":"00060-4406ae30-7e2a-41b9-91bc-08cea83ee434"},"outputs":[],"execution_count":19},{"cell_type":"code","metadata":{"id":"n8RUGIklqxKS","colab_type":"code","colab":{},"cell_id":"00052-65402b1e-d3d4-4efa-9bff-a006a7443971"},"source":"# play with learning rate and epochs\nlearning_rate = ?        # Replace ? with a floating-point number\nepochs = ?             # Replace ? with an integer\noptimizer = RMSprop(learning_rate)\nmodel.compile(loss='mean_squared_error',optimizer=optimizer)    # Compile the model\nmodel.fit(X_train, y_train, epochs=epochs, batch_size=30)       # Fit the model\nmodel.evaluate(X_test, y_test)                                  # Evaluate the model","execution_count":null,"outputs":[]},{"cell_type":"code","source":"combination(0.02, 100)","metadata":{"tags":[],"cell_id":"00062-d265e94c-1888-4558-8f76-ad8e0f227919"},"outputs":[{"name":"stdout","text":"Epoch 1/100\n11/11 [==============================] - 0s 1ms/step - loss: 114.4460\nEpoch 2/100\n11/11 [==============================] - 0s 1ms/step - loss: 72.5732\nEpoch 3/100\n11/11 [==============================] - 0s 2ms/step - loss: 78.2873\nEpoch 4/100\n11/11 [==============================] - 0s 1ms/step - loss: 72.7063\nEpoch 5/100\n11/11 [==============================] - 0s 2ms/step - loss: 70.0100\nEpoch 6/100\n11/11 [==============================] - 0s 1ms/step - loss: 76.0481\nEpoch 7/100\n11/11 [==============================] - 0s 1ms/step - loss: 74.8234\nEpoch 8/100\n11/11 [==============================] - 0s 1ms/step - loss: 72.6066\nEpoch 9/100\n11/11 [==============================] - 0s 1ms/step - loss: 75.8436\nEpoch 10/100\n11/11 [==============================] - 0s 1ms/step - loss: 73.4458\nEpoch 11/100\n11/11 [==============================] - 0s 1ms/step - loss: 62.4364\nEpoch 12/100\n11/11 [==============================] - 0s 1ms/step - loss: 72.4443\nEpoch 13/100\n11/11 [==============================] - 0s 1ms/step - loss: 59.0276\nEpoch 14/100\n11/11 [==============================] - 0s 1ms/step - loss: 65.1122\nEpoch 15/100\n11/11 [==============================] - 0s 1ms/step - loss: 64.9694\nEpoch 16/100\n11/11 [==============================] - 0s 2ms/step - loss: 60.0895\nEpoch 17/100\n11/11 [==============================] - 0s 1ms/step - loss: 62.5698\nEpoch 18/100\n11/11 [==============================] - 0s 1ms/step - loss: 58.2367\nEpoch 19/100\n11/11 [==============================] - 0s 1ms/step - loss: 60.1548\nEpoch 20/100\n11/11 [==============================] - 0s 1ms/step - loss: 53.6123\nEpoch 21/100\n11/11 [==============================] - 0s 2ms/step - loss: 62.1970\nEpoch 22/100\n11/11 [==============================] - ETA: 0s - loss: 46.91 - 0s 2ms/step - loss: 43.4946\nEpoch 23/100\n11/11 [==============================] - 0s 1ms/step - loss: 57.9517\nEpoch 24/100\n11/11 [==============================] - 0s 1ms/step - loss: 49.4254\nEpoch 25/100\n11/11 [==============================] - 0s 1ms/step - loss: 49.8326\nEpoch 26/100\n11/11 [==============================] - 0s 1ms/step - loss: 41.9896\nEpoch 27/100\n11/11 [==============================] - 0s 2ms/step - loss: 50.1337\nEpoch 28/100\n11/11 [==============================] - 0s 1ms/step - loss: 60.3156\nEpoch 29/100\n11/11 [==============================] - 0s 2ms/step - loss: 39.2939\nEpoch 30/100\n11/11 [==============================] - 0s 2ms/step - loss: 55.8296\nEpoch 31/100\n11/11 [==============================] - 0s 2ms/step - loss: 38.9358\nEpoch 32/100\n11/11 [==============================] - 0s 2ms/step - loss: 39.8249\nEpoch 33/100\n11/11 [==============================] - 0s 2ms/step - loss: 44.3889\nEpoch 34/100\n11/11 [==============================] - 0s 1ms/step - loss: 45.6624\nEpoch 35/100\n11/11 [==============================] - 0s 1ms/step - loss: 43.8028\nEpoch 36/100\n11/11 [==============================] - ETA: 0s - loss: 71.96 - 0s 1ms/step - loss: 47.2212\nEpoch 37/100\n11/11 [==============================] - 0s 2ms/step - loss: 50.7579\nEpoch 38/100\n11/11 [==============================] - 0s 2ms/step - loss: 42.1580\nEpoch 39/100\n11/11 [==============================] - 0s 2ms/step - loss: 45.3306\nEpoch 40/100\n11/11 [==============================] - 0s 2ms/step - loss: 40.6797\nEpoch 41/100\n11/11 [==============================] - 0s 1ms/step - loss: 39.3095\nEpoch 42/100\n11/11 [==============================] - 0s 1ms/step - loss: 38.5006\nEpoch 43/100\n11/11 [==============================] - 0s 1ms/step - loss: 46.3904\nEpoch 44/100\n11/11 [==============================] - 0s 1ms/step - loss: 38.6953\nEpoch 45/100\n11/11 [==============================] - 0s 2ms/step - loss: 41.5547\nEpoch 46/100\n11/11 [==============================] - 0s 1ms/step - loss: 45.3614\nEpoch 47/100\n11/11 [==============================] - 0s 1ms/step - loss: 46.5006\nEpoch 48/100\n11/11 [==============================] - 0s 1ms/step - loss: 36.1094\nEpoch 49/100\n11/11 [==============================] - 0s 1ms/step - loss: 45.8222\nEpoch 50/100\n11/11 [==============================] - 0s 1ms/step - loss: 37.6945\nEpoch 51/100\n11/11 [==============================] - 0s 1ms/step - loss: 52.1999\nEpoch 52/100\n11/11 [==============================] - 0s 1ms/step - loss: 35.5502\nEpoch 53/100\n11/11 [==============================] - 0s 1ms/step - loss: 38.6264\nEpoch 54/100\n11/11 [==============================] - 0s 1ms/step - loss: 38.4944\nEpoch 55/100\n11/11 [==============================] - 0s 2ms/step - loss: 40.6406\nEpoch 56/100\n11/11 [==============================] - 0s 3ms/step - loss: 48.2407\nEpoch 57/100\n11/11 [==============================] - 0s 2ms/step - loss: 46.8120\nEpoch 58/100\n11/11 [==============================] - 0s 2ms/step - loss: 34.4721\nEpoch 59/100\n11/11 [==============================] - 0s 2ms/step - loss: 43.4075\nEpoch 60/100\n11/11 [==============================] - 0s 2ms/step - loss: 52.0921\nEpoch 61/100\n11/11 [==============================] - 0s 1ms/step - loss: 32.3179\nEpoch 62/100\n11/11 [==============================] - 0s 1ms/step - loss: 36.2026\nEpoch 63/100\n11/11 [==============================] - 0s 1ms/step - loss: 33.9469\nEpoch 64/100\n11/11 [==============================] - 0s 1ms/step - loss: 46.1080\nEpoch 65/100\n11/11 [==============================] - 0s 1ms/step - loss: 38.6621\nEpoch 66/100\n11/11 [==============================] - 0s 1ms/step - loss: 46.6292\nEpoch 67/100\n11/11 [==============================] - 0s 1ms/step - loss: 29.6479\nEpoch 68/100\n11/11 [==============================] - 0s 1ms/step - loss: 37.2027\nEpoch 69/100\n11/11 [==============================] - 0s 1ms/step - loss: 39.1083\nEpoch 70/100\n11/11 [==============================] - ETA: 0s - loss: 50.82 - 0s 1ms/step - loss: 38.3278\nEpoch 71/100\n11/11 [==============================] - 0s 1ms/step - loss: 41.7671\nEpoch 72/100\n11/11 [==============================] - 0s 2ms/step - loss: 41.6569\nEpoch 73/100\n11/11 [==============================] - 0s 1ms/step - loss: 36.5775\nEpoch 74/100\n11/11 [==============================] - 0s 1ms/step - loss: 38.0393\nEpoch 75/100\n11/11 [==============================] - 0s 2ms/step - loss: 38.5346\nEpoch 76/100\n11/11 [==============================] - 0s 1ms/step - loss: 33.7713\nEpoch 77/100\n11/11 [==============================] - 0s 1ms/step - loss: 37.5903\nEpoch 78/100\n11/11 [==============================] - 0s 1ms/step - loss: 31.8908\nEpoch 79/100\n11/11 [==============================] - 0s 1ms/step - loss: 38.0145\nEpoch 80/100\n11/11 [==============================] - 0s 1ms/step - loss: 28.0043\nEpoch 81/100\n11/11 [==============================] - 0s 1ms/step - loss: 36.6547\nEpoch 82/100\n11/11 [==============================] - 0s 1ms/step - loss: 32.7589\nEpoch 83/100\n11/11 [==============================] - 0s 1ms/step - loss: 46.6934\nEpoch 84/100\n11/11 [==============================] - 0s 1ms/step - loss: 32.8358\nEpoch 85/100\n11/11 [==============================] - 0s 1ms/step - loss: 27.5598\nEpoch 86/100\n11/11 [==============================] - 0s 1ms/step - loss: 29.2219\nEpoch 87/100\n11/11 [==============================] - 0s 1ms/step - loss: 44.6763\nEpoch 88/100\n11/11 [==============================] - 0s 1ms/step - loss: 28.1244\nEpoch 89/100\n11/11 [==============================] - 0s 1ms/step - loss: 41.9219\nEpoch 90/100\n11/11 [==============================] - 0s 1ms/step - loss: 33.0172\nEpoch 91/100\n11/11 [==============================] - 0s 1ms/step - loss: 32.6037\nEpoch 92/100\n11/11 [==============================] - 0s 1ms/step - loss: 48.9622\nEpoch 93/100\n11/11 [==============================] - 0s 1ms/step - loss: 25.4918\nEpoch 94/100\n11/11 [==============================] - 0s 1ms/step - loss: 32.1642\nEpoch 95/100\n11/11 [==============================] - 0s 1ms/step - loss: 43.5587\nEpoch 96/100\n11/11 [==============================] - 0s 1ms/step - loss: 27.1561\nEpoch 97/100\n11/11 [==============================] - 0s 1ms/step - loss: 36.4806\nEpoch 98/100\n11/11 [==============================] - 0s 1ms/step - loss: 30.6799\nEpoch 99/100\n11/11 [==============================] - 0s 1ms/step - loss: 40.3359\nEpoch 100/100\n11/11 [==============================] - 0s 1ms/step - loss: 30.6687\n3/3 [==============================] - 0s 2ms/step - loss: 44.3103\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"**Batch Size**\n\nThe number of examples in a batch.","metadata":{"id":"kcn3gB3Uq7u6","colab_type":"text","cell_id":"00053-1b15a9fc-ab84-48a7-a1c9-46f850705426"}},{"cell_type":"code","metadata":{"id":"DTSP63PTZ8hv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":428},"outputId":"d0a90bd0-9e34-49e4-b351-fa035ae6df22","cell_id":"00054-183b9d49-2b71-49d0-92b9-73b0a48eaf77"},"source":"####################### Complete example to check the performance of the model with different batch size while keeping epochs as 30 and learning rate as 0.01 #######################################\n# define the model\nmodel = Sequential()\nmodel.add(Dense(10, activation='relu', input_shape=(n_features,)))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dense(1))\n\noptimizer = RMSprop(0.1)    # 0.1 is the learning rate\nmodel.compile(loss='mean_squared_error',optimizer=optimizer)    # Compile the model\n\n# fit the model \nmodel.fit(X_train, y_train, epochs=10, batch_size=40, verbose = 1)\n\n# evaluate the model\nprint('The MSE value is: ', model.evaluate(X_test, y_test))","execution_count":21,"outputs":[{"name":"stdout","text":"Epoch 1/10\n9/9 [==============================] - 0s 2ms/step - loss: 12602.9590\nEpoch 2/10\n9/9 [==============================] - 0s 1ms/step - loss: 586.4506\nEpoch 3/10\n9/9 [==============================] - 0s 2ms/step - loss: 567.4573\nEpoch 4/10\n9/9 [==============================] - 0s 2ms/step - loss: 543.1056\nEpoch 5/10\n9/9 [==============================] - 0s 1ms/step - loss: 514.8212\nEpoch 6/10\n9/9 [==============================] - 0s 1ms/step - loss: 483.3157\nEpoch 7/10\n9/9 [==============================] - 0s 1ms/step - loss: 450.3320\nEpoch 8/10\n9/9 [==============================] - 0s 2ms/step - loss: 419.2164\nEpoch 9/10\n9/9 [==============================] - 0s 2ms/step - loss: 390.4407\nEpoch 10/10\n9/9 [==============================] - 0s 2ms/step - loss: 362.2616\n3/3 [==============================] - 0s 2ms/step - loss: 329.0605\nThe MSE value is:  329.0605163574219\n","output_type":"stream"}]},{"cell_type":"markdown","source":"You can see above the cost(loss) value i.e. MSE for batch size 40 while keeping epochs as 10 and learning rate as 0.01","metadata":{"id":"r7xoMXoVaJy5","colab_type":"text","cell_id":"00055-ecd42935-d0d5-4d67-86a1-2b307e7cd2d5"}},{"cell_type":"markdown","source":"### Exercise 4\n\nTest several batch size values to see the impact of varying this value when defining your model.","metadata":{"id":"y9VcjhruiIxd","colab_type":"text","cell_id":"00056-71b7d06f-ab74-4434-9a8b-37bab6c56427"}},{"cell_type":"code","metadata":{"id":"WKik9O5grNa1","colab_type":"code","colab":{},"cell_id":"00057-6ec471ac-2b18-4a5f-9ca7-1e15273c3355"},"source":"# play with batch size\n#Batch size is 100\nlearning_rate = 0.01        \nepochs = 150         \nbatch = 100      # Replace ? with an integer    \noptimizer = RMSprop(learning_rate)\nmodel.compile(loss='mean_squared_error',optimizer=optimizer)    # compile the model\nmodel.fit(X_train, y_train, epochs=epochs, batch_size=batch)     # fit the model\nmodel.evaluate(X_test, y_test)       # Evaluate the model","execution_count":22,"outputs":[{"name":"stdout","text":"Epoch 1/150\n4/4 [==============================] - 0s 1ms/step - loss: 346.0412\nEpoch 2/150\n4/4 [==============================] - 0s 2ms/step - loss: 343.5864\nEpoch 3/150\n4/4 [==============================] - 0s 2ms/step - loss: 341.8426\nEpoch 4/150\n4/4 [==============================] - 0s 2ms/step - loss: 340.3839\nEpoch 5/150\n4/4 [==============================] - 0s 1ms/step - loss: 339.0161\nEpoch 6/150\n4/4 [==============================] - 0s 1ms/step - loss: 337.6636\nEpoch 7/150\n4/4 [==============================] - 0s 1ms/step - loss: 336.3599\nEpoch 8/150\n4/4 [==============================] - 0s 1ms/step - loss: 335.0761\nEpoch 9/150\n4/4 [==============================] - 0s 1ms/step - loss: 333.8033\nEpoch 10/150\n4/4 [==============================] - 0s 1ms/step - loss: 332.5840\nEpoch 11/150\n4/4 [==============================] - 0s 1ms/step - loss: 331.3591\nEpoch 12/150\n4/4 [==============================] - 0s 1ms/step - loss: 330.1411\nEpoch 13/150\n4/4 [==============================] - 0s 1ms/step - loss: 328.9200\nEpoch 14/150\n4/4 [==============================] - 0s 1ms/step - loss: 327.6865\nEpoch 15/150\n4/4 [==============================] - 0s 1ms/step - loss: 326.4294\nEpoch 16/150\n4/4 [==============================] - 0s 1ms/step - loss: 325.1976\nEpoch 17/150\n4/4 [==============================] - 0s 1ms/step - loss: 323.9829\nEpoch 18/150\n4/4 [==============================] - 0s 1ms/step - loss: 322.8109\nEpoch 19/150\n4/4 [==============================] - 0s 2ms/step - loss: 321.6239\nEpoch 20/150\n4/4 [==============================] - 0s 2ms/step - loss: 320.4242\nEpoch 21/150\n4/4 [==============================] - 0s 5ms/step - loss: 319.2117\nEpoch 22/150\n4/4 [==============================] - 0s 1ms/step - loss: 318.0006\nEpoch 23/150\n4/4 [==============================] - 0s 1ms/step - loss: 316.8217\nEpoch 24/150\n4/4 [==============================] - 0s 1ms/step - loss: 315.6129\nEpoch 25/150\n4/4 [==============================] - 0s 1ms/step - loss: 314.4258\nEpoch 26/150\n4/4 [==============================] - 0s 1ms/step - loss: 313.2667\nEpoch 27/150\n4/4 [==============================] - 0s 1ms/step - loss: 312.0708\nEpoch 28/150\n4/4 [==============================] - 0s 1ms/step - loss: 310.8729\nEpoch 29/150\n4/4 [==============================] - 0s 1ms/step - loss: 309.7348\nEpoch 30/150\n4/4 [==============================] - 0s 2ms/step - loss: 308.5894\nEpoch 31/150\n4/4 [==============================] - 0s 1ms/step - loss: 307.4106\nEpoch 32/150\n4/4 [==============================] - 0s 2ms/step - loss: 306.2322\nEpoch 33/150\n4/4 [==============================] - 0s 1ms/step - loss: 305.0528\nEpoch 34/150\n4/4 [==============================] - 0s 1ms/step - loss: 303.9151\nEpoch 35/150\n4/4 [==============================] - 0s 1ms/step - loss: 302.7536\nEpoch 36/150\n4/4 [==============================] - 0s 1ms/step - loss: 301.5305\nEpoch 37/150\n4/4 [==============================] - 0s 1ms/step - loss: 300.4178\nEpoch 38/150\n4/4 [==============================] - 0s 1ms/step - loss: 299.2643\nEpoch 39/150\n4/4 [==============================] - 0s 1ms/step - loss: 298.1357\nEpoch 40/150\n4/4 [==============================] - 0s 2ms/step - loss: 297.0116\nEpoch 41/150\n4/4 [==============================] - 0s 2ms/step - loss: 295.8572\nEpoch 42/150\n4/4 [==============================] - 0s 1ms/step - loss: 294.7551\nEpoch 43/150\n4/4 [==============================] - 0s 2ms/step - loss: 293.6362\nEpoch 44/150\n4/4 [==============================] - 0s 2ms/step - loss: 292.5057\nEpoch 45/150\n4/4 [==============================] - 0s 1ms/step - loss: 291.3904\nEpoch 46/150\n4/4 [==============================] - 0s 2ms/step - loss: 290.2821\nEpoch 47/150\n4/4 [==============================] - 0s 1ms/step - loss: 289.1583\nEpoch 48/150\n4/4 [==============================] - 0s 2ms/step - loss: 288.0583\nEpoch 49/150\n4/4 [==============================] - 0s 2ms/step - loss: 286.9430\nEpoch 50/150\n4/4 [==============================] - 0s 3ms/step - loss: 285.8262\nEpoch 51/150\n4/4 [==============================] - 0s 1ms/step - loss: 284.7018\nEpoch 52/150\n4/4 [==============================] - 0s 2ms/step - loss: 283.6001\nEpoch 53/150\n4/4 [==============================] - 0s 2ms/step - loss: 282.5080\nEpoch 54/150\n4/4 [==============================] - 0s 3ms/step - loss: 281.4167\nEpoch 55/150\n4/4 [==============================] - ETA: 0s - loss: 288.019 - 0s 2ms/step - loss: 280.2934\nEpoch 56/150\n4/4 [==============================] - 0s 2ms/step - loss: 279.1739\nEpoch 57/150\n4/4 [==============================] - 0s 2ms/step - loss: 278.0957\nEpoch 58/150\n4/4 [==============================] - 0s 2ms/step - loss: 277.0135\nEpoch 59/150\n4/4 [==============================] - 0s 2ms/step - loss: 275.9070\nEpoch 60/150\n4/4 [==============================] - 0s 2ms/step - loss: 274.8421\nEpoch 61/150\n4/4 [==============================] - 0s 4ms/step - loss: 273.7863\nEpoch 62/150\n4/4 [==============================] - 0s 2ms/step - loss: 272.6678\nEpoch 63/150\n4/4 [==============================] - 0s 2ms/step - loss: 271.6020\nEpoch 64/150\n4/4 [==============================] - 0s 2ms/step - loss: 270.5657\nEpoch 65/150\n4/4 [==============================] - 0s 2ms/step - loss: 269.5289\nEpoch 66/150\n4/4 [==============================] - 0s 1ms/step - loss: 268.5033\nEpoch 67/150\n4/4 [==============================] - 0s 4ms/step - loss: 267.4347\nEpoch 68/150\n4/4 [==============================] - 0s 4ms/step - loss: 266.3784\nEpoch 69/150\n4/4 [==============================] - 0s 1ms/step - loss: 265.3081\nEpoch 70/150\n4/4 [==============================] - 0s 1ms/step - loss: 264.2597\nEpoch 71/150\n4/4 [==============================] - 0s 2ms/step - loss: 263.2829\nEpoch 72/150\n4/4 [==============================] - 0s 2ms/step - loss: 262.2294\nEpoch 73/150\n4/4 [==============================] - 0s 1ms/step - loss: 261.2032\nEpoch 74/150\n4/4 [==============================] - 0s 1ms/step - loss: 260.1742\nEpoch 75/150\n4/4 [==============================] - 0s 2ms/step - loss: 259.1478\nEpoch 76/150\n4/4 [==============================] - 0s 1ms/step - loss: 258.1331\nEpoch 77/150\n4/4 [==============================] - 0s 2ms/step - loss: 257.1078\nEpoch 78/150\n4/4 [==============================] - 0s 1ms/step - loss: 256.1139\nEpoch 79/150\n4/4 [==============================] - 0s 2ms/step - loss: 255.0779\nEpoch 80/150\n4/4 [==============================] - 0s 2ms/step - loss: 254.0479\nEpoch 81/150\n4/4 [==============================] - 0s 2ms/step - loss: 253.0336\nEpoch 82/150\n4/4 [==============================] - 0s 2ms/step - loss: 251.9971\nEpoch 83/150\n4/4 [==============================] - 0s 1ms/step - loss: 250.9793\nEpoch 84/150\n4/4 [==============================] - 0s 3ms/step - loss: 250.0005\nEpoch 85/150\n4/4 [==============================] - 0s 3ms/step - loss: 249.0345\nEpoch 86/150\n4/4 [==============================] - 0s 2ms/step - loss: 248.0732\nEpoch 87/150\n4/4 [==============================] - 0s 3ms/step - loss: 247.0451\nEpoch 88/150\n4/4 [==============================] - 0s 1ms/step - loss: 246.0390\nEpoch 89/150\n4/4 [==============================] - 0s 1ms/step - loss: 245.0620\nEpoch 90/150\n4/4 [==============================] - 0s 2ms/step - loss: 244.0860\nEpoch 91/150\n4/4 [==============================] - 0s 2ms/step - loss: 243.1038\nEpoch 92/150\n4/4 [==============================] - ETA: 0s - loss: 164.137 - 0s 2ms/step - loss: 242.1427\nEpoch 93/150\n4/4 [==============================] - 0s 3ms/step - loss: 241.1760\nEpoch 94/150\n4/4 [==============================] - 0s 2ms/step - loss: 240.2128\nEpoch 95/150\n4/4 [==============================] - 0s 4ms/step - loss: 239.2387\nEpoch 96/150\n4/4 [==============================] - 0s 2ms/step - loss: 238.2603\nEpoch 97/150\n4/4 [==============================] - 0s 1ms/step - loss: 237.2750\nEpoch 98/150\n4/4 [==============================] - 0s 2ms/step - loss: 236.3509\nEpoch 99/150\n4/4 [==============================] - 0s 3ms/step - loss: 235.3973\nEpoch 100/150\n4/4 [==============================] - 0s 4ms/step - loss: 234.4071\nEpoch 101/150\n4/4 [==============================] - 0s 2ms/step - loss: 233.5038\nEpoch 102/150\n4/4 [==============================] - 0s 2ms/step - loss: 232.5563\nEpoch 103/150\n4/4 [==============================] - 0s 2ms/step - loss: 231.6370\nEpoch 104/150\n4/4 [==============================] - 0s 3ms/step - loss: 230.6773\nEpoch 105/150\n4/4 [==============================] - 0s 2ms/step - loss: 229.7346\nEpoch 106/150\n4/4 [==============================] - ETA: 0s - loss: 210.702 - 0s 2ms/step - loss: 228.8458\nEpoch 107/150\n4/4 [==============================] - 0s 2ms/step - loss: 227.9231\nEpoch 108/150\n4/4 [==============================] - 0s 2ms/step - loss: 227.0158\nEpoch 109/150\n4/4 [==============================] - 0s 2ms/step - loss: 226.0841\nEpoch 110/150\n4/4 [==============================] - 0s 2ms/step - loss: 225.1230\nEpoch 111/150\n4/4 [==============================] - 0s 3ms/step - loss: 224.1798\nEpoch 112/150\n4/4 [==============================] - 0s 2ms/step - loss: 223.2661\nEpoch 113/150\n4/4 [==============================] - 0s 2ms/step - loss: 222.3814\nEpoch 114/150\n4/4 [==============================] - 0s 2ms/step - loss: 221.4652\nEpoch 115/150\n4/4 [==============================] - 0s 2ms/step - loss: 220.5840\nEpoch 116/150\n4/4 [==============================] - 0s 2ms/step - loss: 219.6872\nEpoch 117/150\n4/4 [==============================] - 0s 1ms/step - loss: 218.7799\nEpoch 118/150\n4/4 [==============================] - 0s 1ms/step - loss: 217.9064\nEpoch 119/150\n4/4 [==============================] - 0s 1ms/step - loss: 217.0445\nEpoch 120/150\n4/4 [==============================] - 0s 2ms/step - loss: 216.1391\nEpoch 121/150\n4/4 [==============================] - 0s 1ms/step - loss: 215.2298\nEpoch 122/150\n4/4 [==============================] - 0s 1ms/step - loss: 214.3488\nEpoch 123/150\n4/4 [==============================] - 0s 2ms/step - loss: 213.5038\nEpoch 124/150\n4/4 [==============================] - 0s 1ms/step - loss: 212.6168\nEpoch 125/150\n4/4 [==============================] - 0s 1ms/step - loss: 211.7775\nEpoch 126/150\n4/4 [==============================] - 0s 1ms/step - loss: 210.9105\nEpoch 127/150\n4/4 [==============================] - 0s 1ms/step - loss: 210.0300\nEpoch 128/150\n4/4 [==============================] - 0s 1ms/step - loss: 209.1779\nEpoch 129/150\n4/4 [==============================] - 0s 1ms/step - loss: 208.2667\nEpoch 130/150\n4/4 [==============================] - 0s 1ms/step - loss: 207.3981\nEpoch 131/150\n4/4 [==============================] - 0s 1ms/step - loss: 206.5721\nEpoch 132/150\n4/4 [==============================] - 0s 1ms/step - loss: 205.6921\nEpoch 133/150\n4/4 [==============================] - 0s 1ms/step - loss: 204.8834\nEpoch 134/150\n4/4 [==============================] - 0s 1ms/step - loss: 204.0810\nEpoch 135/150\n4/4 [==============================] - 0s 1ms/step - loss: 203.2886\nEpoch 136/150\n4/4 [==============================] - 0s 1ms/step - loss: 202.4230\nEpoch 137/150\n4/4 [==============================] - 0s 2ms/step - loss: 201.5899\nEpoch 138/150\n4/4 [==============================] - 0s 2ms/step - loss: 200.7804\nEpoch 139/150\n4/4 [==============================] - 0s 1ms/step - loss: 199.8986\nEpoch 140/150\n4/4 [==============================] - 0s 1ms/step - loss: 199.0592\nEpoch 141/150\n4/4 [==============================] - 0s 1ms/step - loss: 198.2718\nEpoch 142/150\n4/4 [==============================] - 0s 2ms/step - loss: 197.4734\nEpoch 143/150\n4/4 [==============================] - 0s 2ms/step - loss: 196.6894\nEpoch 144/150\n4/4 [==============================] - 0s 1ms/step - loss: 195.8927\nEpoch 145/150\n4/4 [==============================] - 0s 2ms/step - loss: 195.1054\nEpoch 146/150\n4/4 [==============================] - 0s 3ms/step - loss: 194.2992\nEpoch 147/150\n4/4 [==============================] - 0s 4ms/step - loss: 193.4693\nEpoch 148/150\n4/4 [==============================] - 0s 1ms/step - loss: 192.7020\nEpoch 149/150\n4/4 [==============================] - 0s 2ms/step - loss: 191.9068\nEpoch 150/150\n4/4 [==============================] - 0s 2ms/step - loss: 191.1309\nWARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fba95193268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n3/3 [==============================] - 0s 1ms/step - loss: 171.9248\n","output_type":"stream"},{"output_type":"execute_result","execution_count":22,"data":{"text/plain":"171.92477416992188"},"metadata":{}}]},{"cell_type":"code","source":"# play with batch size\n#Batch size is 300\nlearning_rate = 0.01        \nepochs = 150         \nbatch = 300      # Replace ? with an integer    \noptimizer = RMSprop(learning_rate)\nmodel.compile(loss='mean_squared_error',optimizer=optimizer)    # compile the model\nmodel.fit(X_train, y_train, epochs=epochs, batch_size=batch)     # fit the model\nmodel.evaluate(X_test, y_test)       # Evaluate the model","metadata":{"tags":[],"cell_id":"00068-189c8bd8-6115-4aa3-b0ff-25aea861aeec"},"outputs":[{"name":"stdout","text":"Epoch 1/150\n2/2 [==============================] - 0s 3ms/step - loss: 190.5565\nEpoch 2/150\n2/2 [==============================] - 0s 2ms/step - loss: 189.4572\nEpoch 3/150\n2/2 [==============================] - 0s 2ms/step - loss: 188.7118\nEpoch 4/150\n2/2 [==============================] - 0s 2ms/step - loss: 188.1808\nEpoch 5/150\n2/2 [==============================] - 0s 2ms/step - loss: 187.6976\nEpoch 6/150\n2/2 [==============================] - 0s 1ms/step - loss: 187.1988\nEpoch 7/150\n2/2 [==============================] - 0s 1ms/step - loss: 186.7388\nEpoch 8/150\n2/2 [==============================] - 0s 2ms/step - loss: 186.2865\nEpoch 9/150\n2/2 [==============================] - 0s 3ms/step - loss: 185.8422\nEpoch 10/150\n2/2 [==============================] - 0s 1ms/step - loss: 185.4644\nEpoch 11/150\n2/2 [==============================] - 0s 2ms/step - loss: 185.0803\nEpoch 12/150\n2/2 [==============================] - 0s 1ms/step - loss: 184.7009\nEpoch 13/150\n2/2 [==============================] - 0s 1ms/step - loss: 184.3223\nEpoch 14/150\n2/2 [==============================] - 0s 2ms/step - loss: 183.9320\nEpoch 15/150\n2/2 [==============================] - 0s 2ms/step - loss: 183.5025\nEpoch 16/150\n2/2 [==============================] - 0s 1ms/step - loss: 183.0931\nEpoch 17/150\n2/2 [==============================] - 0s 1ms/step - loss: 182.7021\nEpoch 18/150\n2/2 [==============================] - 0s 1ms/step - loss: 182.3594\nEpoch 19/150\n2/2 [==============================] - 0s 2ms/step - loss: 182.0038\nEpoch 20/150\n2/2 [==============================] - 0s 2ms/step - loss: 181.6349\nEpoch 21/150\n2/2 [==============================] - 0s 2ms/step - loss: 181.2495\nEpoch 22/150\n2/2 [==============================] - 0s 2ms/step - loss: 180.8552\nEpoch 23/150\n2/2 [==============================] - 0s 3ms/step - loss: 180.4942\nEpoch 24/150\n2/2 [==============================] - 0s 2ms/step - loss: 180.1012\nEpoch 25/150\n2/2 [==============================] - 0s 3ms/step - loss: 179.7286\nEpoch 26/150\n2/2 [==============================] - 0s 1ms/step - loss: 179.3868\nEpoch 27/150\n2/2 [==============================] - 0s 2ms/step - loss: 179.0030\nEpoch 28/150\n2/2 [==============================] - 0s 3ms/step - loss: 178.6115\nEpoch 29/150\n2/2 [==============================] - 0s 2ms/step - loss: 178.2850\nEpoch 30/150\n2/2 [==============================] - 0s 1ms/step - loss: 177.9577\nEpoch 31/150\n2/2 [==============================] - 0s 3ms/step - loss: 177.5850\nEpoch 32/150\n2/2 [==============================] - 0s 2ms/step - loss: 177.2017\nEpoch 33/150\n2/2 [==============================] - 0s 2ms/step - loss: 176.8137\nEpoch 34/150\n2/2 [==============================] - 0s 1ms/step - loss: 176.4746\nEpoch 35/150\n2/2 [==============================] - 0s 2ms/step - loss: 176.1062\nEpoch 36/150\n2/2 [==============================] - 0s 2ms/step - loss: 175.6514\nEpoch 37/150\n2/2 [==============================] - 0s 1ms/step - loss: 175.3209\nEpoch 38/150\n2/2 [==============================] - 0s 1ms/step - loss: 174.9468\nEpoch 39/150\n2/2 [==============================] - 0s 2ms/step - loss: 174.6025\nEpoch 40/150\n2/2 [==============================] - 0s 3ms/step - loss: 174.2620\nEpoch 41/150\n2/2 [==============================] - 0s 2ms/step - loss: 173.8831\nEpoch 42/150\n2/2 [==============================] - 0s 1ms/step - loss: 173.5669\nEpoch 43/150\n2/2 [==============================] - 0s 1ms/step - loss: 173.2320\nEpoch 44/150\n2/2 [==============================] - 0s 1ms/step - loss: 172.8798\nEpoch 45/150\n2/2 [==============================] - 0s 3ms/step - loss: 172.5405\nEpoch 46/150\n2/2 [==============================] - 0s 2ms/step - loss: 172.2093\nEpoch 47/150\n2/2 [==============================] - 0s 2ms/step - loss: 171.8542\nEpoch 48/150\n2/2 [==============================] - 0s 2ms/step - loss: 171.5216\nEpoch 49/150\n2/2 [==============================] - 0s 1ms/step - loss: 171.1678\nEpoch 50/150\n2/2 [==============================] - 0s 9ms/step - loss: 170.8106\nEpoch 51/150\n2/2 [==============================] - 0s 2ms/step - loss: 170.4319\nEpoch 52/150\n2/2 [==============================] - 0s 3ms/step - loss: 170.0789\nEpoch 53/150\n2/2 [==============================] - 0s 3ms/step - loss: 169.7366\nEpoch 54/150\n2/2 [==============================] - 0s 3ms/step - loss: 169.3942\nEpoch 55/150\n2/2 [==============================] - 0s 2ms/step - loss: 169.0060\nEpoch 56/150\n2/2 [==============================] - 0s 4ms/step - loss: 168.6128\nEpoch 57/150\n2/2 [==============================] - 0s 2ms/step - loss: 168.2659\nEpoch 58/150\n2/2 [==============================] - 0s 2ms/step - loss: 167.9139\nEpoch 59/150\n2/2 [==============================] - 0s 2ms/step - loss: 167.5294\nEpoch 60/150\n2/2 [==============================] - 0s 2ms/step - loss: 167.1924\nEpoch 61/150\n2/2 [==============================] - 0s 2ms/step - loss: 166.8676\nEpoch 62/150\n2/2 [==============================] - 0s 2ms/step - loss: 166.4657\nEpoch 63/150\n2/2 [==============================] - 0s 2ms/step - loss: 166.1166\nEpoch 64/150\n2/2 [==============================] - 0s 2ms/step - loss: 165.8021\nEpoch 65/150\n2/2 [==============================] - 0s 2ms/step - loss: 165.4866\nEpoch 66/150\n2/2 [==============================] - 0s 2ms/step - loss: 165.1885\nEpoch 67/150\n2/2 [==============================] - 0s 1ms/step - loss: 164.8423\nEpoch 68/150\n2/2 [==============================] - 0s 1ms/step - loss: 164.5046\nEpoch 69/150\n2/2 [==============================] - 0s 2ms/step - loss: 164.1440\nEpoch 70/150\n2/2 [==============================] - 0s 1ms/step - loss: 163.7960\nEpoch 71/150\n2/2 [==============================] - 0s 2ms/step - loss: 163.5387\nEpoch 72/150\n2/2 [==============================] - 0s 2ms/step - loss: 163.1959\nEpoch 73/150\n2/2 [==============================] - 0s 2ms/step - loss: 162.8784\nEpoch 74/150\n2/2 [==============================] - 0s 1ms/step - loss: 162.5561\nEpoch 75/150\n2/2 [==============================] - 0s 2ms/step - loss: 162.2339\nEpoch 76/150\n2/2 [==============================] - 0s 1ms/step - loss: 161.9228\nEpoch 77/150\n2/2 [==============================] - 0s 2ms/step - loss: 161.5913\nEpoch 78/150\n2/2 [==============================] - 0s 1ms/step - loss: 161.3009\nEpoch 79/150\n2/2 [==============================] - 0s 1ms/step - loss: 160.9562\nEpoch 80/150\n2/2 [==============================] - 0s 2ms/step - loss: 160.6073\nEpoch 81/150\n2/2 [==============================] - 0s 3ms/step - loss: 160.2724\nEpoch 82/150\n2/2 [==============================] - 0s 2ms/step - loss: 159.9067\nEpoch 83/150\n2/2 [==============================] - 0s 1ms/step - loss: 159.5579\nEpoch 84/150\n2/2 [==============================] - 0s 1ms/step - loss: 159.2501\nEpoch 85/150\n2/2 [==============================] - 0s 1ms/step - loss: 158.9596\nEpoch 86/150\n2/2 [==============================] - 0s 2ms/step - loss: 158.6785\nEpoch 87/150\n2/2 [==============================] - 0s 2ms/step - loss: 158.3177\nEpoch 88/150\n2/2 [==============================] - 0s 2ms/step - loss: 157.9726\nEpoch 89/150\n2/2 [==============================] - 0s 1ms/step - loss: 157.6571\nEpoch 90/150\n2/2 [==============================] - 0s 2ms/step - loss: 157.3392\nEpoch 91/150\n2/2 [==============================] - 0s 1ms/step - loss: 157.0117\nEpoch 92/150\n2/2 [==============================] - 0s 2ms/step - loss: 156.7119\nEpoch 93/150\n2/2 [==============================] - 0s 2ms/step - loss: 156.4055\nEpoch 94/150\n2/2 [==============================] - 0s 3ms/step - loss: 156.0981\nEpoch 95/150\n2/2 [==============================] - 0s 1ms/step - loss: 155.7718\nEpoch 96/150\n2/2 [==============================] - 0s 1ms/step - loss: 155.4386\nEpoch 97/150\n2/2 [==============================] - 0s 3ms/step - loss: 155.0873\nEpoch 98/150\n2/2 [==============================] - 0s 1ms/step - loss: 154.8032\nEpoch 99/150\n2/2 [==============================] - 0s 3ms/step - loss: 154.4878\nEpoch 100/150\n2/2 [==============================] - 0s 3ms/step - loss: 154.1211\nEpoch 101/150\n2/2 [==============================] - 0s 2ms/step - loss: 153.8566\nEpoch 102/150\n2/2 [==============================] - 0s 1ms/step - loss: 153.5418\nEpoch 103/150\n2/2 [==============================] - 0s 1ms/step - loss: 153.2565\nEpoch 104/150\n2/2 [==============================] - 0s 1ms/step - loss: 152.9235\nEpoch 105/150\n2/2 [==============================] - 0s 2ms/step - loss: 152.6056\nEpoch 106/150\n2/2 [==============================] - 0s 1ms/step - loss: 152.3519\nEpoch 107/150\n2/2 [==============================] - 0s 1ms/step - loss: 152.0521\nEpoch 108/150\n2/2 [==============================] - 0s 1ms/step - loss: 151.7678\nEpoch 109/150\n2/2 [==============================] - 0s 1ms/step - loss: 151.4550\nEpoch 110/150\n2/2 [==============================] - 0s 1ms/step - loss: 151.0969\nEpoch 111/150\n2/2 [==============================] - 0s 2ms/step - loss: 150.7508\nEpoch 112/150\n2/2 [==============================] - 0s 1ms/step - loss: 150.4332\nEpoch 113/150\n2/2 [==============================] - 0s 1ms/step - loss: 150.1505\nEpoch 114/150\n2/2 [==============================] - 0s 2ms/step - loss: 149.8311\nEpoch 115/150\n2/2 [==============================] - 0s 1ms/step - loss: 149.5532\nEpoch 116/150\n2/2 [==============================] - 0s 2ms/step - loss: 149.2598\nEpoch 117/150\n2/2 [==============================] - 0s 1ms/step - loss: 148.9449\nEpoch 118/150\n2/2 [==============================] - 0s 1ms/step - loss: 148.6691\nEpoch 119/150\n2/2 [==============================] - 0s 2ms/step - loss: 148.4112\nEpoch 120/150\n2/2 [==============================] - 0s 3ms/step - loss: 148.0963\nEpoch 121/150\n2/2 [==============================] - 0s 2ms/step - loss: 147.7678\nEpoch 122/150\n2/2 [==============================] - 0s 1ms/step - loss: 147.4640\nEpoch 123/150\n2/2 [==============================] - ETA: 0s - loss: 146.563 - 0s 1ms/step - loss: 147.2074\nEpoch 124/150\n2/2 [==============================] - 0s 1ms/step - loss: 146.9002\nEpoch 125/150\n2/2 [==============================] - 0s 2ms/step - loss: 146.6465\nEpoch 126/150\n2/2 [==============================] - 0s 2ms/step - loss: 146.3614\nEpoch 127/150\n2/2 [==============================] - 0s 1ms/step - loss: 146.0547\nEpoch 128/150\n2/2 [==============================] - 0s 1ms/step - loss: 145.7795\nEpoch 129/150\n2/2 [==============================] - 0s 1ms/step - loss: 145.4235\nEpoch 130/150\n2/2 [==============================] - 0s 1ms/step - loss: 145.1059\nEpoch 131/150\n2/2 [==============================] - 0s 2ms/step - loss: 144.8413\nEpoch 132/150\n2/2 [==============================] - 0s 2ms/step - loss: 144.5106\nEpoch 133/150\n2/2 [==============================] - 0s 2ms/step - loss: 144.2567\nEpoch 134/150\n2/2 [==============================] - 0s 2ms/step - loss: 144.0148\nEpoch 135/150\n2/2 [==============================] - 0s 1ms/step - loss: 143.7945\nEpoch 136/150\n2/2 [==============================] - 0s 2ms/step - loss: 143.4873\nEpoch 137/150\n2/2 [==============================] - 0s 2ms/step - loss: 143.2054\nEpoch 138/150\n2/2 [==============================] - 0s 2ms/step - loss: 142.9501\nEpoch 139/150\n2/2 [==============================] - 0s 2ms/step - loss: 142.5999\nEpoch 140/150\n2/2 [==============================] - 0s 3ms/step - loss: 142.2881\nEpoch 141/150\n2/2 [==============================] - 0s 1ms/step - loss: 142.0362\nEpoch 142/150\n2/2 [==============================] - 0s 2ms/step - loss: 141.7750\nEpoch 143/150\n2/2 [==============================] - 0s 2ms/step - loss: 141.5328\nEpoch 144/150\n2/2 [==============================] - 0s 1ms/step - loss: 141.2771\nEpoch 145/150\n2/2 [==============================] - 0s 2ms/step - loss: 141.0334\nEpoch 146/150\n2/2 [==============================] - 0s 1ms/step - loss: 140.7677\nEpoch 147/150\n2/2 [==============================] - 0s 2ms/step - loss: 140.4633\nEpoch 148/150\n2/2 [==============================] - 0s 2ms/step - loss: 140.2273\nEpoch 149/150\n2/2 [==============================] - 0s 2ms/step - loss: 139.9579\nEpoch 150/150\n2/2 [==============================] - 0s 1ms/step - loss: 139.7114\nWARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fbae0474510> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n3/3 [==============================] - 0s 4ms/step - loss: 120.5020\n","output_type":"stream"},{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"120.50199890136719"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"# play with batch size\n#Batch size is 500\nlearning_rate = 0.01        \nepochs = 150         \nbatch = 500      # Replace ? with an integer    \noptimizer = RMSprop(learning_rate)\nmodel.compile(loss='mean_squared_error',optimizer=optimizer)    # compile the model\nmodel.fit(X_train, y_train, epochs=epochs, batch_size=batch)     # fit the model\nmodel.evaluate(X_test, y_test)       # Evaluate the model","metadata":{"tags":[],"cell_id":"00069-2cb6119f-cb46-45a8-8212-0f073afbeead"},"outputs":[{"name":"stdout","text":"Epoch 1/150\n1/1 [==============================] - 0s 2ms/step - loss: 139.4995\nEpoch 2/150\n1/1 [==============================] - 0s 2ms/step - loss: 139.0592\nEpoch 3/150\n1/1 [==============================] - 0s 1ms/step - loss: 138.7417\nEpoch 4/150\n1/1 [==============================] - 0s 1ms/step - loss: 138.4771\nEpoch 5/150\n1/1 [==============================] - 0s 2ms/step - loss: 138.2430\nEpoch 6/150\n1/1 [==============================] - 0s 1ms/step - loss: 138.0292\nEpoch 7/150\n1/1 [==============================] - 0s 2ms/step - loss: 137.8298\nEpoch 8/150\n1/1 [==============================] - 0s 2ms/step - loss: 137.6414\nEpoch 9/150\n1/1 [==============================] - 0s 1ms/step - loss: 137.4615\nEpoch 10/150\n1/1 [==============================] - 0s 1ms/step - loss: 137.2885\nEpoch 11/150\n1/1 [==============================] - 0s 1ms/step - loss: 137.1211\nEpoch 12/150\n1/1 [==============================] - 0s 2ms/step - loss: 136.9583\nEpoch 13/150\n1/1 [==============================] - 0s 1ms/step - loss: 136.7994\nEpoch 14/150\n1/1 [==============================] - 0s 1ms/step - loss: 136.6440\nEpoch 15/150\n1/1 [==============================] - 0s 1ms/step - loss: 136.4914\nEpoch 16/150\n1/1 [==============================] - 0s 1ms/step - loss: 136.3413\nEpoch 17/150\n1/1 [==============================] - 0s 1ms/step - loss: 136.1934\nEpoch 18/150\n1/1 [==============================] - 0s 1ms/step - loss: 136.0474\nEpoch 19/150\n1/1 [==============================] - 0s 2ms/step - loss: 135.9032\nEpoch 20/150\n1/1 [==============================] - 0s 2ms/step - loss: 135.7604\nEpoch 21/150\n1/1 [==============================] - 0s 1ms/step - loss: 135.6190\nEpoch 22/150\n1/1 [==============================] - 0s 2ms/step - loss: 135.4789\nEpoch 23/150\n1/1 [==============================] - 0s 1ms/step - loss: 135.3398\nEpoch 24/150\n1/1 [==============================] - 0s 2ms/step - loss: 135.2017\nEpoch 25/150\n1/1 [==============================] - 0s 2ms/step - loss: 135.0645\nEpoch 26/150\n1/1 [==============================] - 0s 2ms/step - loss: 134.9282\nEpoch 27/150\n1/1 [==============================] - 0s 1ms/step - loss: 134.7926\nEpoch 28/150\n1/1 [==============================] - 0s 1ms/step - loss: 134.6577\nEpoch 29/150\n1/1 [==============================] - 0s 2ms/step - loss: 134.5234\nEpoch 30/150\n1/1 [==============================] - 0s 2ms/step - loss: 134.3897\nEpoch 31/150\n1/1 [==============================] - 0s 1ms/step - loss: 134.2565\nEpoch 32/150\n1/1 [==============================] - 0s 1ms/step - loss: 134.1239\nEpoch 33/150\n1/1 [==============================] - 0s 2ms/step - loss: 133.9918\nEpoch 34/150\n1/1 [==============================] - 0s 1ms/step - loss: 133.8600\nEpoch 35/150\n1/1 [==============================] - 0s 1ms/step - loss: 133.7288\nEpoch 36/150\n1/1 [==============================] - 0s 1ms/step - loss: 133.5979\nEpoch 37/150\n1/1 [==============================] - 0s 2ms/step - loss: 133.4674\nEpoch 38/150\n1/1 [==============================] - 0s 1ms/step - loss: 133.3372\nEpoch 39/150\n1/1 [==============================] - 0s 2ms/step - loss: 133.2074\nEpoch 40/150\n1/1 [==============================] - 0s 2ms/step - loss: 133.0780\nEpoch 41/150\n1/1 [==============================] - 0s 2ms/step - loss: 132.9488\nEpoch 42/150\n1/1 [==============================] - 0s 1ms/step - loss: 132.8199\nEpoch 43/150\n1/1 [==============================] - 0s 2ms/step - loss: 132.6914\nEpoch 44/150\n1/1 [==============================] - 0s 2ms/step - loss: 132.5631\nEpoch 45/150\n1/1 [==============================] - 0s 6ms/step - loss: 132.4351\nEpoch 46/150\n1/1 [==============================] - 0s 5ms/step - loss: 132.3073\nEpoch 47/150\n1/1 [==============================] - 0s 2ms/step - loss: 132.1799\nEpoch 48/150\n1/1 [==============================] - 0s 2ms/step - loss: 132.0526\nEpoch 49/150\n1/1 [==============================] - 0s 1ms/step - loss: 131.9256\nEpoch 50/150\n1/1 [==============================] - 0s 2ms/step - loss: 131.7989\nEpoch 51/150\n1/1 [==============================] - 0s 2ms/step - loss: 131.6724\nEpoch 52/150\n1/1 [==============================] - 0s 1ms/step - loss: 131.5461\nEpoch 53/150\n1/1 [==============================] - 0s 2ms/step - loss: 131.4201\nEpoch 54/150\n1/1 [==============================] - 0s 1ms/step - loss: 131.2942\nEpoch 55/150\n1/1 [==============================] - 0s 4ms/step - loss: 131.1686\nEpoch 56/150\n1/1 [==============================] - 0s 5ms/step - loss: 131.0433\nEpoch 57/150\n1/1 [==============================] - 0s 2ms/step - loss: 130.9181\nEpoch 58/150\n1/1 [==============================] - 0s 1ms/step - loss: 130.7932\nEpoch 59/150\n1/1 [==============================] - 0s 2ms/step - loss: 130.6684\nEpoch 60/150\n1/1 [==============================] - 0s 5ms/step - loss: 130.5439\nEpoch 61/150\n1/1 [==============================] - 0s 2ms/step - loss: 130.4196\nEpoch 62/150\n1/1 [==============================] - 0s 2ms/step - loss: 130.2955\nEpoch 63/150\n1/1 [==============================] - 0s 3ms/step - loss: 130.1716\nEpoch 64/150\n1/1 [==============================] - 0s 2ms/step - loss: 130.0480\nEpoch 65/150\n1/1 [==============================] - 0s 2ms/step - loss: 129.9245\nEpoch 66/150\n1/1 [==============================] - 0s 1ms/step - loss: 129.8012\nEpoch 67/150\n1/1 [==============================] - 0s 1ms/step - loss: 129.6781\nEpoch 68/150\n1/1 [==============================] - 0s 1ms/step - loss: 129.5553\nEpoch 69/150\n1/1 [==============================] - 0s 2ms/step - loss: 129.4327\nEpoch 70/150\n1/1 [==============================] - 0s 6ms/step - loss: 129.3102\nEpoch 71/150\n1/1 [==============================] - 0s 3ms/step - loss: 129.1880\nEpoch 72/150\n1/1 [==============================] - 0s 2ms/step - loss: 129.0659\nEpoch 73/150\n1/1 [==============================] - 0s 5ms/step - loss: 128.9441\nEpoch 74/150\n1/1 [==============================] - 0s 2ms/step - loss: 128.8224\nEpoch 75/150\n1/1 [==============================] - 0s 2ms/step - loss: 128.7010\nEpoch 76/150\n1/1 [==============================] - 0s 2ms/step - loss: 128.5797\nEpoch 77/150\n1/1 [==============================] - 0s 3ms/step - loss: 128.4587\nEpoch 78/150\n1/1 [==============================] - 0s 2ms/step - loss: 128.3378\nEpoch 79/150\n1/1 [==============================] - 0s 2ms/step - loss: 128.2172\nEpoch 80/150\n1/1 [==============================] - 0s 2ms/step - loss: 128.0968\nEpoch 81/150\n1/1 [==============================] - 0s 2ms/step - loss: 127.9765\nEpoch 82/150\n1/1 [==============================] - 0s 2ms/step - loss: 127.8564\nEpoch 83/150\n1/1 [==============================] - 0s 2ms/step - loss: 127.7366\nEpoch 84/150\n1/1 [==============================] - 0s 2ms/step - loss: 127.6169\nEpoch 85/150\n1/1 [==============================] - 0s 2ms/step - loss: 127.4975\nEpoch 86/150\n1/1 [==============================] - 0s 1ms/step - loss: 127.3782\nEpoch 87/150\n1/1 [==============================] - 0s 3ms/step - loss: 127.2592\nEpoch 88/150\n1/1 [==============================] - 0s 2ms/step - loss: 127.1403\nEpoch 89/150\n1/1 [==============================] - 0s 4ms/step - loss: 127.0216\nEpoch 90/150\n1/1 [==============================] - 0s 2ms/step - loss: 126.9032\nEpoch 91/150\n1/1 [==============================] - 0s 2ms/step - loss: 126.7849\nEpoch 92/150\n1/1 [==============================] - 0s 2ms/step - loss: 126.6668\nEpoch 93/150\n1/1 [==============================] - 0s 2ms/step - loss: 126.5490\nEpoch 94/150\n1/1 [==============================] - 0s 2ms/step - loss: 126.4313\nEpoch 95/150\n1/1 [==============================] - 0s 2ms/step - loss: 126.3138\nEpoch 96/150\n1/1 [==============================] - 0s 2ms/step - loss: 126.1965\nEpoch 97/150\n1/1 [==============================] - 0s 7ms/step - loss: 126.0794\nEpoch 98/150\n1/1 [==============================] - 0s 1ms/step - loss: 125.9625\nEpoch 99/150\n1/1 [==============================] - 0s 5ms/step - loss: 125.8459\nEpoch 100/150\n1/1 [==============================] - 0s 2ms/step - loss: 125.7294\nEpoch 101/150\n1/1 [==============================] - 0s 1ms/step - loss: 125.6131\nEpoch 102/150\n1/1 [==============================] - 0s 2ms/step - loss: 125.4970\nEpoch 103/150\n1/1 [==============================] - 0s 1ms/step - loss: 125.3810\nEpoch 104/150\n1/1 [==============================] - 0s 1ms/step - loss: 125.2654\nEpoch 105/150\n1/1 [==============================] - 0s 2ms/step - loss: 125.1498\nEpoch 106/150\n1/1 [==============================] - 0s 2ms/step - loss: 125.0345\nEpoch 107/150\n1/1 [==============================] - 0s 2ms/step - loss: 124.9194\nEpoch 108/150\n1/1 [==============================] - 0s 1ms/step - loss: 124.8045\nEpoch 109/150\n1/1 [==============================] - 0s 2ms/step - loss: 124.6898\nEpoch 110/150\n1/1 [==============================] - 0s 1ms/step - loss: 124.5752\nEpoch 111/150\n1/1 [==============================] - 0s 2ms/step - loss: 124.4609\nEpoch 112/150\n1/1 [==============================] - 0s 2ms/step - loss: 124.3468\nEpoch 113/150\n1/1 [==============================] - 0s 2ms/step - loss: 124.2329\nEpoch 114/150\n1/1 [==============================] - 0s 1ms/step - loss: 124.1191\nEpoch 115/150\n1/1 [==============================] - 0s 2ms/step - loss: 124.0056\nEpoch 116/150\n1/1 [==============================] - 0s 3ms/step - loss: 123.8922\nEpoch 117/150\n1/1 [==============================] - 0s 1ms/step - loss: 123.7791\nEpoch 118/150\n1/1 [==============================] - 0s 2ms/step - loss: 123.6661\nEpoch 119/150\n1/1 [==============================] - 0s 1ms/step - loss: 123.5534\nEpoch 120/150\n1/1 [==============================] - 0s 1ms/step - loss: 123.4408\nEpoch 121/150\n1/1 [==============================] - 0s 1ms/step - loss: 123.3285\nEpoch 122/150\n1/1 [==============================] - 0s 1ms/step - loss: 123.2163\nEpoch 123/150\n1/1 [==============================] - 0s 1ms/step - loss: 123.1043\nEpoch 124/150\n1/1 [==============================] - 0s 2ms/step - loss: 122.9926\nEpoch 125/150\n1/1 [==============================] - 0s 1ms/step - loss: 122.8810\nEpoch 126/150\n1/1 [==============================] - 0s 1ms/step - loss: 122.7696\nEpoch 127/150\n1/1 [==============================] - 0s 1ms/step - loss: 122.6584\nEpoch 128/150\n1/1 [==============================] - 0s 1ms/step - loss: 122.5475\nEpoch 129/150\n1/1 [==============================] - 0s 1ms/step - loss: 122.4367\nEpoch 130/150\n1/1 [==============================] - 0s 1ms/step - loss: 122.3261\nEpoch 131/150\n1/1 [==============================] - 0s 2ms/step - loss: 122.2157\nEpoch 132/150\n1/1 [==============================] - 0s 3ms/step - loss: 122.1055\nEpoch 133/150\n1/1 [==============================] - 0s 1ms/step - loss: 121.9955\nEpoch 134/150\n1/1 [==============================] - 0s 1ms/step - loss: 121.8857\nEpoch 135/150\n1/1 [==============================] - 0s 1ms/step - loss: 121.7761\nEpoch 136/150\n1/1 [==============================] - 0s 1ms/step - loss: 121.6667\nEpoch 137/150\n1/1 [==============================] - 0s 1ms/step - loss: 121.5575\nEpoch 138/150\n1/1 [==============================] - 0s 1ms/step - loss: 121.4485\nEpoch 139/150\n1/1 [==============================] - 0s 2ms/step - loss: 121.3397\nEpoch 140/150\n1/1 [==============================] - 0s 2ms/step - loss: 121.2311\nEpoch 141/150\n1/1 [==============================] - 0s 2ms/step - loss: 121.1227\nEpoch 142/150\n1/1 [==============================] - 0s 1ms/step - loss: 121.0144\nEpoch 143/150\n1/1 [==============================] - 0s 2ms/step - loss: 120.9064\nEpoch 144/150\n1/1 [==============================] - 0s 1ms/step - loss: 120.7986\nEpoch 145/150\n1/1 [==============================] - 0s 4ms/step - loss: 120.6909\nEpoch 146/150\n1/1 [==============================] - 0s 1ms/step - loss: 120.5835\nEpoch 147/150\n1/1 [==============================] - 0s 1ms/step - loss: 120.4762\nEpoch 148/150\n1/1 [==============================] - 0s 2ms/step - loss: 120.3692\nEpoch 149/150\n1/1 [==============================] - 0s 1ms/step - loss: 120.2623\nEpoch 150/150\n1/1 [==============================] - 0s 2ms/step - loss: 120.1557\nWARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fbad83cf158> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n3/3 [==============================] - 0s 2ms/step - loss: 100.8878\n","output_type":"stream"},{"output_type":"execute_result","execution_count":24,"data":{"text/plain":"100.8877944946289"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"# play with batch size\n#Batch size is 800\nlearning_rate = 0.01        \nepochs = 150         \nbatch = 800      # Replace ? with an integer    \noptimizer = RMSprop(learning_rate)\nmodel.compile(loss='mean_squared_error',optimizer=optimizer)    # compile the model\nmodel.fit(X_train, y_train, epochs=epochs, batch_size=batch)     # fit the model\nmodel.evaluate(X_test, y_test)       # Evaluate the model","metadata":{"tags":[],"cell_id":"00070-c8949631-3612-439a-91bd-0e383a621a03"},"outputs":[{"name":"stdout","text":"Epoch 1/150\n1/1 [==============================] - 0s 2ms/step - loss: 120.0492\nEpoch 2/150\n1/1 [==============================] - 0s 1ms/step - loss: 119.7084\nEpoch 3/150\n1/1 [==============================] - 0s 1ms/step - loss: 119.4630\nEpoch 4/150\n1/1 [==============================] - 0s 1ms/step - loss: 119.2587\nEpoch 5/150\n1/1 [==============================] - 0s 1ms/step - loss: 119.0782\nEpoch 6/150\n1/1 [==============================] - 0s 1ms/step - loss: 118.9134\nEpoch 7/150\n1/1 [==============================] - 0s 1ms/step - loss: 118.7600\nEpoch 8/150\n1/1 [==============================] - 0s 973us/step - loss: 118.6150\nEpoch 9/150\n1/1 [==============================] - 0s 986us/step - loss: 118.4768\nEpoch 10/150\n1/1 [==============================] - 0s 1ms/step - loss: 118.3439\nEpoch 11/150\n1/1 [==============================] - 0s 1ms/step - loss: 118.2154\nEpoch 12/150\n1/1 [==============================] - 0s 1ms/step - loss: 118.0905\nEpoch 13/150\n1/1 [==============================] - 0s 1ms/step - loss: 117.9688\nEpoch 14/150\n1/1 [==============================] - 0s 1ms/step - loss: 117.8497\nEpoch 15/150\n1/1 [==============================] - 0s 1ms/step - loss: 117.7328\nEpoch 16/150\n1/1 [==============================] - 0s 1ms/step - loss: 117.6180\nEpoch 17/150\n1/1 [==============================] - 0s 1ms/step - loss: 117.5049\nEpoch 18/150\n1/1 [==============================] - 0s 1ms/step - loss: 117.3933\nEpoch 19/150\n1/1 [==============================] - 0s 1ms/step - loss: 117.2831\nEpoch 20/150\n1/1 [==============================] - 0s 1ms/step - loss: 117.1742\nEpoch 21/150\n1/1 [==============================] - 0s 1ms/step - loss: 117.0663\nEpoch 22/150\n1/1 [==============================] - 0s 1ms/step - loss: 116.9594\nEpoch 23/150\n1/1 [==============================] - 0s 1ms/step - loss: 116.8534\nEpoch 24/150\n1/1 [==============================] - 0s 999us/step - loss: 116.7482\nEpoch 25/150\n1/1 [==============================] - 0s 1ms/step - loss: 116.6437\nEpoch 26/150\n1/1 [==============================] - 0s 1ms/step - loss: 116.5399\nEpoch 27/150\n1/1 [==============================] - 0s 1ms/step - loss: 116.4368\nEpoch 28/150\n1/1 [==============================] - 0s 1ms/step - loss: 116.3342\nEpoch 29/150\n1/1 [==============================] - 0s 1ms/step - loss: 116.2322\nEpoch 30/150\n1/1 [==============================] - 0s 1ms/step - loss: 116.1307\nEpoch 31/150\n1/1 [==============================] - 0s 2ms/step - loss: 116.0296\nEpoch 32/150\n1/1 [==============================] - 0s 993us/step - loss: 115.9290\nEpoch 33/150\n1/1 [==============================] - 0s 1ms/step - loss: 115.8288\nEpoch 34/150\n1/1 [==============================] - 0s 1ms/step - loss: 115.7290\nEpoch 35/150\n1/1 [==============================] - 0s 1ms/step - loss: 115.6296\nEpoch 36/150\n1/1 [==============================] - 0s 1ms/step - loss: 115.5305\nEpoch 37/150\n1/1 [==============================] - 0s 1ms/step - loss: 115.4317\nEpoch 38/150\n1/1 [==============================] - 0s 2ms/step - loss: 115.3333\nEpoch 39/150\n1/1 [==============================] - 0s 2ms/step - loss: 115.2351\nEpoch 40/150\n1/1 [==============================] - 0s 1ms/step - loss: 115.1373\nEpoch 41/150\n1/1 [==============================] - 0s 1ms/step - loss: 115.0398\nEpoch 42/150\n1/1 [==============================] - 0s 1ms/step - loss: 114.9425\nEpoch 43/150\n1/1 [==============================] - 0s 1ms/step - loss: 114.8455\nEpoch 44/150\n1/1 [==============================] - 0s 2ms/step - loss: 114.7488\nEpoch 45/150\n1/1 [==============================] - 0s 1ms/step - loss: 114.6523\nEpoch 46/150\n1/1 [==============================] - 0s 1ms/step - loss: 114.5561\nEpoch 47/150\n1/1 [==============================] - 0s 1ms/step - loss: 114.4601\nEpoch 48/150\n1/1 [==============================] - 0s 1ms/step - loss: 114.3643\nEpoch 49/150\n1/1 [==============================] - 0s 1ms/step - loss: 114.2688\nEpoch 50/150\n1/1 [==============================] - 0s 1ms/step - loss: 114.1736\nEpoch 51/150\n1/1 [==============================] - 0s 1ms/step - loss: 114.0785\nEpoch 52/150\n1/1 [==============================] - 0s 2ms/step - loss: 113.9837\nEpoch 53/150\n1/1 [==============================] - 0s 2ms/step - loss: 113.8891\nEpoch 54/150\n1/1 [==============================] - 0s 4ms/step - loss: 113.7947\nEpoch 55/150\n1/1 [==============================] - 0s 1ms/step - loss: 113.7005\nEpoch 56/150\n1/1 [==============================] - 0s 1ms/step - loss: 113.6065\nEpoch 57/150\n1/1 [==============================] - 0s 1ms/step - loss: 113.5128\nEpoch 58/150\n1/1 [==============================] - 0s 1ms/step - loss: 113.4192\nEpoch 59/150\n1/1 [==============================] - 0s 2ms/step - loss: 113.3259\nEpoch 60/150\n1/1 [==============================] - 0s 1ms/step - loss: 113.2328\nEpoch 61/150\n1/1 [==============================] - 0s 2ms/step - loss: 113.1399\nEpoch 62/150\n1/1 [==============================] - 0s 1ms/step - loss: 113.0471\nEpoch 63/150\n1/1 [==============================] - 0s 2ms/step - loss: 112.9547\nEpoch 64/150\n1/1 [==============================] - 0s 3ms/step - loss: 112.8624\nEpoch 65/150\n1/1 [==============================] - 0s 1ms/step - loss: 112.7702\nEpoch 66/150\n1/1 [==============================] - 0s 2ms/step - loss: 112.6784\nEpoch 67/150\n1/1 [==============================] - 0s 2ms/step - loss: 112.5867\nEpoch 68/150\n1/1 [==============================] - 0s 1ms/step - loss: 112.4952\nEpoch 69/150\n1/1 [==============================] - 0s 1ms/step - loss: 112.4039\nEpoch 70/150\n1/1 [==============================] - 0s 1ms/step - loss: 112.3128\nEpoch 71/150\n1/1 [==============================] - 0s 993us/step - loss: 112.2219\nEpoch 72/150\n1/1 [==============================] - 0s 1ms/step - loss: 112.1312\nEpoch 73/150\n1/1 [==============================] - 0s 1ms/step - loss: 112.0407\nEpoch 74/150\n1/1 [==============================] - 0s 1ms/step - loss: 111.9504\nEpoch 75/150\n1/1 [==============================] - 0s 1ms/step - loss: 111.8603\nEpoch 76/150\n1/1 [==============================] - 0s 1ms/step - loss: 111.7705\nEpoch 77/150\n1/1 [==============================] - 0s 2ms/step - loss: 111.6807\nEpoch 78/150\n1/1 [==============================] - 0s 1ms/step - loss: 111.5913\nEpoch 79/150\n1/1 [==============================] - 0s 2ms/step - loss: 111.5020\nEpoch 80/150\n1/1 [==============================] - 0s 1ms/step - loss: 111.4129\nEpoch 81/150\n1/1 [==============================] - 0s 1ms/step - loss: 111.3240\nEpoch 82/150\n1/1 [==============================] - 0s 1ms/step - loss: 111.2353\nEpoch 83/150\n1/1 [==============================] - 0s 1ms/step - loss: 111.1467\nEpoch 84/150\n1/1 [==============================] - 0s 2ms/step - loss: 111.0584\nEpoch 85/150\n1/1 [==============================] - 0s 9ms/step - loss: 110.9703\nEpoch 86/150\n1/1 [==============================] - 0s 2ms/step - loss: 110.8824\nEpoch 87/150\n1/1 [==============================] - 0s 2ms/step - loss: 110.7947\nEpoch 88/150\n1/1 [==============================] - 0s 4ms/step - loss: 110.7071\nEpoch 89/150\n1/1 [==============================] - 0s 2ms/step - loss: 110.6198\nEpoch 90/150\n1/1 [==============================] - 0s 2ms/step - loss: 110.5327\nEpoch 91/150\n1/1 [==============================] - 0s 1ms/step - loss: 110.4457\nEpoch 92/150\n1/1 [==============================] - 0s 6ms/step - loss: 110.3590\nEpoch 93/150\n1/1 [==============================] - 0s 3ms/step - loss: 110.2724\nEpoch 94/150\n1/1 [==============================] - 0s 1ms/step - loss: 110.1861\nEpoch 95/150\n1/1 [==============================] - 0s 1ms/step - loss: 110.0999\nEpoch 96/150\n1/1 [==============================] - 0s 2ms/step - loss: 110.0140\nEpoch 97/150\n1/1 [==============================] - 0s 2ms/step - loss: 109.9282\nEpoch 98/150\n1/1 [==============================] - 0s 1ms/step - loss: 109.8427\nEpoch 99/150\n1/1 [==============================] - 0s 2ms/step - loss: 109.7573\nEpoch 100/150\n1/1 [==============================] - 0s 1ms/step - loss: 109.6721\nEpoch 101/150\n1/1 [==============================] - 0s 1ms/step - loss: 109.5871\nEpoch 102/150\n1/1 [==============================] - 0s 2ms/step - loss: 109.5024\nEpoch 103/150\n1/1 [==============================] - 0s 2ms/step - loss: 109.4178\nEpoch 104/150\n1/1 [==============================] - 0s 1ms/step - loss: 109.3334\nEpoch 105/150\n1/1 [==============================] - 0s 1ms/step - loss: 109.2492\nEpoch 106/150\n1/1 [==============================] - 0s 1ms/step - loss: 109.1652\nEpoch 107/150\n1/1 [==============================] - 0s 1ms/step - loss: 109.0814\nEpoch 108/150\n1/1 [==============================] - 0s 1ms/step - loss: 108.9978\nEpoch 109/150\n1/1 [==============================] - 0s 1ms/step - loss: 108.9144\nEpoch 110/150\n1/1 [==============================] - 0s 2ms/step - loss: 108.8312\nEpoch 111/150\n1/1 [==============================] - 0s 1ms/step - loss: 108.7482\nEpoch 112/150\n1/1 [==============================] - 0s 2ms/step - loss: 108.6654\nEpoch 113/150\n1/1 [==============================] - 0s 1ms/step - loss: 108.5827\nEpoch 114/150\n1/1 [==============================] - 0s 1ms/step - loss: 108.5003\nEpoch 115/150\n1/1 [==============================] - 0s 1ms/step - loss: 108.4181\nEpoch 116/150\n1/1 [==============================] - 0s 1ms/step - loss: 108.3360\nEpoch 117/150\n1/1 [==============================] - 0s 1ms/step - loss: 108.2542\nEpoch 118/150\n1/1 [==============================] - 0s 2ms/step - loss: 108.1726\nEpoch 119/150\n1/1 [==============================] - 0s 1ms/step - loss: 108.0911\nEpoch 120/150\n1/1 [==============================] - 0s 1ms/step - loss: 108.0098\nEpoch 121/150\n1/1 [==============================] - 0s 2ms/step - loss: 107.9288\nEpoch 122/150\n1/1 [==============================] - 0s 1ms/step - loss: 107.8479\nEpoch 123/150\n1/1 [==============================] - 0s 2ms/step - loss: 107.7673\nEpoch 124/150\n1/1 [==============================] - 0s 1ms/step - loss: 107.6868\nEpoch 125/150\n1/1 [==============================] - 0s 2ms/step - loss: 107.6065\nEpoch 126/150\n1/1 [==============================] - 0s 2ms/step - loss: 107.5264\nEpoch 127/150\n1/1 [==============================] - 0s 1ms/step - loss: 107.4465\nEpoch 128/150\n1/1 [==============================] - 0s 1ms/step - loss: 107.3669\nEpoch 129/150\n1/1 [==============================] - 0s 2ms/step - loss: 107.2874\nEpoch 130/150\n1/1 [==============================] - 0s 1ms/step - loss: 107.2081\nEpoch 131/150\n1/1 [==============================] - 0s 2ms/step - loss: 107.1290\nEpoch 132/150\n1/1 [==============================] - 0s 1ms/step - loss: 107.0501\nEpoch 133/150\n1/1 [==============================] - 0s 6ms/step - loss: 106.9713\nEpoch 134/150\n1/1 [==============================] - 0s 2ms/step - loss: 106.8928\nEpoch 135/150\n1/1 [==============================] - 0s 1ms/step - loss: 106.8145\nEpoch 136/150\n1/1 [==============================] - 0s 1ms/step - loss: 106.7364\nEpoch 137/150\n1/1 [==============================] - 0s 1ms/step - loss: 106.6584\nEpoch 138/150\n1/1 [==============================] - 0s 1ms/step - loss: 106.5807\nEpoch 139/150\n1/1 [==============================] - 0s 2ms/step - loss: 106.5032\nEpoch 140/150\n1/1 [==============================] - 0s 2ms/step - loss: 106.4258\nEpoch 141/150\n1/1 [==============================] - 0s 1ms/step - loss: 106.3487\nEpoch 142/150\n1/1 [==============================] - 0s 2ms/step - loss: 106.2717\nEpoch 143/150\n1/1 [==============================] - 0s 1ms/step - loss: 106.1950\nEpoch 144/150\n1/1 [==============================] - 0s 1ms/step - loss: 106.1184\nEpoch 145/150\n1/1 [==============================] - 0s 5ms/step - loss: 106.0420\nEpoch 146/150\n1/1 [==============================] - 0s 6ms/step - loss: 105.9659\nEpoch 147/150\n1/1 [==============================] - 0s 3ms/step - loss: 105.8899\nEpoch 148/150\n1/1 [==============================] - 0s 2ms/step - loss: 105.8141\nEpoch 149/150\n1/1 [==============================] - 0s 1ms/step - loss: 105.7385\nEpoch 150/150\n1/1 [==============================] - 0s 1ms/step - loss: 105.6631\nWARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fba94ecba60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n3/3 [==============================] - 0s 2ms/step - loss: 86.2632\n","output_type":"stream"},{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"86.26323699951172"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"# play with batch size\n#Batch size is 1000\nlearning_rate = 0.01        \nepochs = 150         \nbatch = 1000      # Replace ? with an integer    \noptimizer = RMSprop(learning_rate)\nmodel.compile(loss='mean_squared_error',optimizer=optimizer)    # compile the model\nmodel.fit(X_train, y_train, epochs=epochs, batch_size=batch)     # fit the model\nmodel.evaluate(X_test, y_test)       # Evaluate the model","metadata":{"tags":[],"cell_id":"00071-2cb91873-3797-49f9-83f5-8c005fd38150"},"outputs":[{"name":"stdout","text":"Epoch 1/150\n1/1 [==============================] - 0s 2ms/step - loss: 105.5880\nEpoch 2/150\n1/1 [==============================] - 0s 1ms/step - loss: 105.3460\nEpoch 3/150\n1/1 [==============================] - 0s 1ms/step - loss: 105.1724\nEpoch 4/150\n1/1 [==============================] - 0s 1ms/step - loss: 105.0281\nEpoch 5/150\n1/1 [==============================] - 0s 1ms/step - loss: 104.9009\nEpoch 6/150\n1/1 [==============================] - 0s 1ms/step - loss: 104.7851\nEpoch 7/150\n1/1 [==============================] - 0s 1ms/step - loss: 104.6773\nEpoch 8/150\n1/1 [==============================] - 0s 1ms/step - loss: 104.5756\nEpoch 9/150\n1/1 [==============================] - 0s 1ms/step - loss: 104.4788\nEpoch 10/150\n1/1 [==============================] - 0s 997us/step - loss: 104.3859\nEpoch 11/150\n1/1 [==============================] - 0s 1ms/step - loss: 104.2961\nEpoch 12/150\n1/1 [==============================] - 0s 2ms/step - loss: 104.2090\nEpoch 13/150\n1/1 [==============================] - 0s 1ms/step - loss: 104.1241\nEpoch 14/150\n1/1 [==============================] - 0s 2ms/step - loss: 104.0412\nEpoch 15/150\n1/1 [==============================] - 0s 1ms/step - loss: 103.9599\nEpoch 16/150\n1/1 [==============================] - 0s 1ms/step - loss: 103.8802\nEpoch 17/150\n1/1 [==============================] - 0s 2ms/step - loss: 103.8017\nEpoch 18/150\n1/1 [==============================] - 0s 2ms/step - loss: 103.7244\nEpoch 19/150\n1/1 [==============================] - 0s 2ms/step - loss: 103.6481\nEpoch 20/150\n1/1 [==============================] - 0s 1ms/step - loss: 103.5727\nEpoch 21/150\n1/1 [==============================] - 0s 1ms/step - loss: 103.4982\nEpoch 22/150\n1/1 [==============================] - 0s 2ms/step - loss: 103.4244\nEpoch 23/150\n1/1 [==============================] - 0s 2ms/step - loss: 103.3513\nEpoch 24/150\n1/1 [==============================] - 0s 1ms/step - loss: 103.2788\nEpoch 25/150\n1/1 [==============================] - 0s 996us/step - loss: 103.2069\nEpoch 26/150\n1/1 [==============================] - 0s 1ms/step - loss: 103.1356\nEpoch 27/150\n1/1 [==============================] - 0s 1ms/step - loss: 103.0647\nEpoch 28/150\n1/1 [==============================] - 0s 1ms/step - loss: 102.9943\nEpoch 29/150\n1/1 [==============================] - 0s 2ms/step - loss: 102.9244\nEpoch 30/150\n1/1 [==============================] - 0s 1ms/step - loss: 102.8549\nEpoch 31/150\n1/1 [==============================] - 0s 1ms/step - loss: 102.7857\nEpoch 32/150\n1/1 [==============================] - 0s 1ms/step - loss: 102.7169\nEpoch 33/150\n1/1 [==============================] - 0s 1ms/step - loss: 102.6485\nEpoch 34/150\n1/1 [==============================] - 0s 1ms/step - loss: 102.5804\nEpoch 35/150\n1/1 [==============================] - 0s 1ms/step - loss: 102.5126\nEpoch 36/150\n1/1 [==============================] - 0s 2ms/step - loss: 102.4452\nEpoch 37/150\n1/1 [==============================] - 0s 1ms/step - loss: 102.3780\nEpoch 38/150\n1/1 [==============================] - 0s 2ms/step - loss: 102.3111\nEpoch 39/150\n1/1 [==============================] - 0s 2ms/step - loss: 102.2444\nEpoch 40/150\n1/1 [==============================] - 0s 1ms/step - loss: 102.1781\nEpoch 41/150\n1/1 [==============================] - 0s 1ms/step - loss: 102.1120\nEpoch 42/150\n1/1 [==============================] - 0s 1ms/step - loss: 102.0461\nEpoch 43/150\n1/1 [==============================] - 0s 2ms/step - loss: 101.9805\nEpoch 44/150\n1/1 [==============================] - 0s 1ms/step - loss: 101.9152\nEpoch 45/150\n1/1 [==============================] - 0s 1ms/step - loss: 101.8500\nEpoch 46/150\n1/1 [==============================] - 0s 1ms/step - loss: 101.7852\nEpoch 47/150\n1/1 [==============================] - 0s 2ms/step - loss: 101.7205\nEpoch 48/150\n1/1 [==============================] - 0s 1ms/step - loss: 101.6560\nEpoch 49/150\n1/1 [==============================] - 0s 1ms/step - loss: 101.5918\nEpoch 50/150\n1/1 [==============================] - 0s 1ms/step - loss: 101.5278\nEpoch 51/150\n1/1 [==============================] - 0s 1ms/step - loss: 101.4640\nEpoch 52/150\n1/1 [==============================] - 0s 2ms/step - loss: 101.4005\nEpoch 53/150\n1/1 [==============================] - 0s 2ms/step - loss: 101.3371\nEpoch 54/150\n1/1 [==============================] - 0s 1ms/step - loss: 101.2740\nEpoch 55/150\n1/1 [==============================] - 0s 2ms/step - loss: 101.2110\nEpoch 56/150\n1/1 [==============================] - 0s 1ms/step - loss: 101.1483\nEpoch 57/150\n1/1 [==============================] - 0s 13ms/step - loss: 101.0858\nEpoch 58/150\n1/1 [==============================] - 0s 2ms/step - loss: 101.0235\nEpoch 59/150\n1/1 [==============================] - 0s 1ms/step - loss: 100.9613\nEpoch 60/150\n1/1 [==============================] - 0s 1ms/step - loss: 100.8994\nEpoch 61/150\n1/1 [==============================] - 0s 991us/step - loss: 100.8377\nEpoch 62/150\n1/1 [==============================] - 0s 1ms/step - loss: 100.7762\nEpoch 63/150\n1/1 [==============================] - 0s 1ms/step - loss: 100.7149\nEpoch 64/150\n1/1 [==============================] - 0s 1ms/step - loss: 100.6538\nEpoch 65/150\n1/1 [==============================] - 0s 1ms/step - loss: 100.5929\nEpoch 66/150\n1/1 [==============================] - 0s 2ms/step - loss: 100.5322\nEpoch 67/150\n1/1 [==============================] - 0s 1ms/step - loss: 100.4717\nEpoch 68/150\n1/1 [==============================] - 0s 2ms/step - loss: 100.4113\nEpoch 69/150\n1/1 [==============================] - 0s 1ms/step - loss: 100.3512\nEpoch 70/150\n1/1 [==============================] - 0s 1ms/step - loss: 100.2913\nEpoch 71/150\n1/1 [==============================] - 0s 1ms/step - loss: 100.2316\nEpoch 72/150\n1/1 [==============================] - 0s 1ms/step - loss: 100.1720\nEpoch 73/150\n1/1 [==============================] - 0s 2ms/step - loss: 100.1127\nEpoch 74/150\n1/1 [==============================] - 0s 1ms/step - loss: 100.0536\nEpoch 75/150\n1/1 [==============================] - 0s 1ms/step - loss: 99.9946\nEpoch 76/150\n1/1 [==============================] - 0s 1ms/step - loss: 99.9359\nEpoch 77/150\n1/1 [==============================] - 0s 1ms/step - loss: 99.8773\nEpoch 78/150\n1/1 [==============================] - 0s 1ms/step - loss: 99.8190\nEpoch 79/150\n1/1 [==============================] - 0s 1ms/step - loss: 99.7608\nEpoch 80/150\n1/1 [==============================] - 0s 1ms/step - loss: 99.7029\nEpoch 81/150\n1/1 [==============================] - 0s 2ms/step - loss: 99.6451\nEpoch 82/150\n1/1 [==============================] - 0s 3ms/step - loss: 99.5875\nEpoch 83/150\n1/1 [==============================] - 0s 1ms/step - loss: 99.5302\nEpoch 84/150\n1/1 [==============================] - 0s 1ms/step - loss: 99.4730\nEpoch 85/150\n1/1 [==============================] - 0s 1ms/step - loss: 99.4160\nEpoch 86/150\n1/1 [==============================] - 0s 1ms/step - loss: 99.3592\nEpoch 87/150\n1/1 [==============================] - 0s 1ms/step - loss: 99.3026\nEpoch 88/150\n1/1 [==============================] - 0s 1ms/step - loss: 99.2462\nEpoch 89/150\n1/1 [==============================] - 0s 1ms/step - loss: 99.1900\nEpoch 90/150\n1/1 [==============================] - 0s 1ms/step - loss: 99.1339\nEpoch 91/150\n1/1 [==============================] - 0s 1ms/step - loss: 99.0781\nEpoch 92/150\n1/1 [==============================] - 0s 1ms/step - loss: 99.0225\nEpoch 93/150\n1/1 [==============================] - 0s 1ms/step - loss: 98.9670\nEpoch 94/150\n1/1 [==============================] - 0s 2ms/step - loss: 98.9118\nEpoch 95/150\n1/1 [==============================] - 0s 1ms/step - loss: 98.8568\nEpoch 96/150\n1/1 [==============================] - 0s 1ms/step - loss: 98.8019\nEpoch 97/150\n1/1 [==============================] - 0s 1ms/step - loss: 98.7472\nEpoch 98/150\n1/1 [==============================] - 0s 1ms/step - loss: 98.6928\nEpoch 99/150\n1/1 [==============================] - 0s 1ms/step - loss: 98.6385\nEpoch 100/150\n1/1 [==============================] - 0s 2ms/step - loss: 98.5844\nEpoch 101/150\n1/1 [==============================] - 0s 1ms/step - loss: 98.5305\nEpoch 102/150\n1/1 [==============================] - 0s 1ms/step - loss: 98.4769\nEpoch 103/150\n1/1 [==============================] - 0s 1ms/step - loss: 98.4233\nEpoch 104/150\n1/1 [==============================] - 0s 2ms/step - loss: 98.3701\nEpoch 105/150\n1/1 [==============================] - 0s 1ms/step - loss: 98.3169\nEpoch 106/150\n1/1 [==============================] - 0s 2ms/step - loss: 98.2640\nEpoch 107/150\n1/1 [==============================] - 0s 1ms/step - loss: 98.2113\nEpoch 108/150\n1/1 [==============================] - 0s 2ms/step - loss: 98.1588\nEpoch 109/150\n1/1 [==============================] - 0s 1ms/step - loss: 98.1064\nEpoch 110/150\n1/1 [==============================] - 0s 1ms/step - loss: 98.0543\nEpoch 111/150\n1/1 [==============================] - 0s 1ms/step - loss: 98.0023\nEpoch 112/150\n1/1 [==============================] - 0s 1ms/step - loss: 97.9506\nEpoch 113/150\n1/1 [==============================] - 0s 1ms/step - loss: 97.8990\nEpoch 114/150\n1/1 [==============================] - 0s 1ms/step - loss: 97.8476\nEpoch 115/150\n1/1 [==============================] - 0s 3ms/step - loss: 97.7965\nEpoch 116/150\n1/1 [==============================] - 0s 1ms/step - loss: 97.7455\nEpoch 117/150\n1/1 [==============================] - 0s 1ms/step - loss: 97.6947\nEpoch 118/150\n1/1 [==============================] - 0s 1ms/step - loss: 97.6441\nEpoch 119/150\n1/1 [==============================] - 0s 2ms/step - loss: 97.5937\nEpoch 120/150\n1/1 [==============================] - 0s 1ms/step - loss: 97.5435\nEpoch 121/150\n1/1 [==============================] - 0s 4ms/step - loss: 97.4935\nEpoch 122/150\n1/1 [==============================] - 0s 2ms/step - loss: 97.4436\nEpoch 123/150\n1/1 [==============================] - 0s 2ms/step - loss: 97.3940\nEpoch 124/150\n1/1 [==============================] - 0s 2ms/step - loss: 97.3446\nEpoch 125/150\n1/1 [==============================] - 0s 2ms/step - loss: 97.2953\nEpoch 126/150\n1/1 [==============================] - 0s 2ms/step - loss: 97.2463\nEpoch 127/150\n1/1 [==============================] - 0s 3ms/step - loss: 97.1974\nEpoch 128/150\n1/1 [==============================] - 0s 3ms/step - loss: 97.1487\nEpoch 129/150\n1/1 [==============================] - 0s 2ms/step - loss: 97.1002\nEpoch 130/150\n1/1 [==============================] - 0s 2ms/step - loss: 97.0520\nEpoch 131/150\n1/1 [==============================] - 0s 1ms/step - loss: 97.0039\nEpoch 132/150\n1/1 [==============================] - 0s 1ms/step - loss: 96.9560\nEpoch 133/150\n1/1 [==============================] - 0s 1ms/step - loss: 96.9083\nEpoch 134/150\n1/1 [==============================] - 0s 1ms/step - loss: 96.8608\nEpoch 135/150\n1/1 [==============================] - 0s 2ms/step - loss: 96.8135\nEpoch 136/150\n1/1 [==============================] - 0s 2ms/step - loss: 96.7663\nEpoch 137/150\n1/1 [==============================] - 0s 2ms/step - loss: 96.7194\nEpoch 138/150\n1/1 [==============================] - 0s 1ms/step - loss: 96.6727\nEpoch 139/150\n1/1 [==============================] - 0s 2ms/step - loss: 96.6261\nEpoch 140/150\n1/1 [==============================] - 0s 3ms/step - loss: 96.5797\nEpoch 141/150\n1/1 [==============================] - 0s 2ms/step - loss: 96.5336\nEpoch 142/150\n1/1 [==============================] - 0s 2ms/step - loss: 96.4876\nEpoch 143/150\n1/1 [==============================] - 0s 1ms/step - loss: 96.4418\nEpoch 144/150\n1/1 [==============================] - 0s 1ms/step - loss: 96.3962\nEpoch 145/150\n1/1 [==============================] - 0s 3ms/step - loss: 96.3509\nEpoch 146/150\n1/1 [==============================] - 0s 4ms/step - loss: 96.3057\nEpoch 147/150\n1/1 [==============================] - 0s 1ms/step - loss: 96.2607\nEpoch 148/150\n1/1 [==============================] - 0s 2ms/step - loss: 96.2158\nEpoch 149/150\n1/1 [==============================] - 0s 2ms/step - loss: 96.1712\nEpoch 150/150\n1/1 [==============================] - 0s 2ms/step - loss: 96.1268\nWARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fbad842d268> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n3/3 [==============================] - 0s 2ms/step - loss: 76.5960\n","output_type":"stream"},{"output_type":"execute_result","execution_count":26,"data":{"text/plain":"76.5960464477539"},"metadata":{}}],"execution_count":26},{"cell_type":"markdown","source":"### Here we see that batch size affects the performance of the model, as the batch size increases the cost function reduces","metadata":{"tags":[],"cell_id":"00072-218edefb-1c06-49c1-9b77-53f79275c890"}},{"cell_type":"markdown","source":"#### **Summary of hyperparameter tuning**\nMost machine learning problems require a lot of hyperparameter tuning. Unfortunately, we can't provide concrete tuning rules for every model. Lowering the learning rate can help one model converge efficiently but make another model converge much too slowly. You must experiment to find the best set of hyperparameters for your dataset. That said, here are a few rules of thumb:\n\n*  Training loss should steadily decrease, steeply at first, and then more slowly until the slope of the curve reaches or approaches zero.\n*  If the training loss does not converge, train for more epochs.\n*  If the training loss decreases too slowly, increase the learning rate. Note that setting the learning rate too high may also prevent training loss from converging.\n*  If the training loss varies wildly (that is, the training loss jumps around), decrease the learning rate.\n*  Lowering the learning rate while increasing the number of epochs or the batch size is often a good combination.\n*  Setting the batch size to a very small batch number can also cause instability. First, try large batch size values. Then, decrease the batch size until you see degradation.\n*  For real-world datasets consisting of a very large number of examples, the entire dataset might not fit into memory. In such cases, you'll need to reduce the batch size to enable a batch to fit into memory.\n\nRemember: the ideal combination of hyperparameters is data dependent, so you must always experiment and verify.","metadata":{"id":"wQV6zkKXrwR0","colab_type":"text","cell_id":"00058-cebd619c-2f59-47e9-8d1a-59ef5fb4fd36"}},{"cell_type":"markdown","source":"#### 5. Make a Prediction\nMaking a prediction is the final step in the life-cycle. It is why we wanted the model in the first place.\n\nIt requires you have new data for which a prediction is required, e.g. where you do not have the output values.\n\nFrom an API perspective, you simply call a function to make a prediction of a class label, probability, or numerical value: whatever you designed your model to predict.\n\nWe have our new test data located at the given github location:\n\nhttps://raw.githubusercontent.com/dphi-official/Datasets/master/Boston_Housing/Testing_set_boston.csv\n\n","metadata":{"id":"3wYL0qp4fTv-","colab_type":"text","cell_id":"00059-3e78ebcb-1585-4f06-9357-3c3c59463a49"}},{"cell_type":"code","metadata":{"id":"s-fWHtrMe8U_","colab_type":"code","colab":{},"cell_id":"00060-7ae47b57-d125-4cec-b769-019d894268db"},"source":"# Load new test data\nnew_test_data = pd.read_csv('https://raw.githubusercontent.com/dphi-official/Datasets/master/Boston_Housing/Testing_set_boston.csv')","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"crRYu0YtiO5G","colab_type":"code","colab":{},"cell_id":"00061-20944c4a-250c-48f4-8411-f933f9c5626f"},"source":"# make a prediction\nmodel.predict(new_test_data)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n**Congratulations! You have successfully build your first deep learning model and predicted the output (i.e. MEDV) of new test data.**","metadata":{"id":"UuoFhNRrn623","colab_type":"text","cell_id":"00062-fbb4e332-89a4-44ef-97b4-e0f3b9387e76"}},{"cell_type":"markdown","source":"#### Resources\n*  [https://machinelearningmastery.com/tensorflow-tutorial-deep-learning-with-tf-keras/](https://machinelearningmastery.com/tensorflow-tutorial-deep-learning-with-tf-keras/)\n*  [https://heartbeat.fritz.ai/linear-regression-using-keras-and-python-7cee2819a60c](https://heartbeat.fritz.ai/linear-regression-using-keras-and-python-7cee2819a60c)\n*  Google Machine Learning Crash Course","metadata":{"id":"c-SLnxZxp02g","colab_type":"text","cell_id":"00063-0921ccbd-ce48-436e-9f0d-a30ef04bfd2b"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Linear_Regression with tf.keras.ipynb","provenance":[],"collapsed_sections":[],"include_colab_link":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"deepnote_notebook_id":"d43982a8-64ec-441a-b2f2-897750d9ec82","deepnote_execution_queue":[]}}